{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sE0oWeYIBFOp",
    "outputId": "834967c2-9d98-493c-f135-7f63658e151e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/speaker_trainer/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-04-26 22:46:05.570517: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing the dtw module. When using in academic works please cite:\n",
      "  T. Giorgino. Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package.\n",
      "  J. Stat. Soft., doi:10.18637/jss.v031.i07.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/echuraev/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/echuraev/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet34\n",
    "import pytorch_lightning as pl\n",
    "from sklearn.metrics import fbeta_score\n",
    "import torch.nn.functional as F\n",
    "from fastai.vision.all import *\n",
    "import os\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from emotiefflib.facial_analysis import EmotiEffLibRecognizer\n",
    "import numpy as np\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "from cvzone.PoseModule import PoseDetector\n",
    "import joblib\n",
    "from joblib import load\n",
    "import string\n",
    "import moviepy.editor as mp_editor\n",
    "import whisper_timestamped\n",
    "from aniemore.models import HuggingFaceModel\n",
    "from aniemore.recognizers.voice import VoiceRecognizer\n",
    "from moviepy.audio.io.AudioFileClip import AudioFileClip\n",
    "from pydub import AudioSegment\n",
    "import wave\n",
    "import nltk\n",
    "from nltk import word_tokenize, FreqDist\n",
    "import noisereduce as nr\n",
    "import librosa\n",
    "import scipy.io.wavfile as wavf\n",
    "from moviepy.audio.io.AudioFileClip import AudioFileClip\n",
    "from pystoi import stoi\n",
    "#from google.colab import drive\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import display\n",
    "import onnxruntime as ort\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7pmONhWWAMYr"
   },
   "source": [
    "## Computer Vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Xzxw49MgAWAe"
   },
   "outputs": [],
   "source": [
    "class LabelSmoothingBCEWithLogitsLossFlat(BCEWithLogitsLossFlat):\n",
    "    \"\"\"\n",
    "    Modified loss function.\n",
    "    \"\"\"\n",
    "    def init(self, eps:float=0.1, **kwargs):\n",
    "        self.eps = eps\n",
    "        super().init(thresh=0.2, **kwargs)\n",
    "\n",
    "    def call(self, inp, targ, **kwargs):\n",
    "        # https://www.kaggle.com/c/siim-isic-melanoma-classification/discussion/166833#929222\n",
    "        targ_smooth = targ.float() * (1. - self.eps) + 0.5 * self.eps\n",
    "        return super().call(inp, targ_smooth, **kwargs)\n",
    "\n",
    "class CustomResNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Tuned resnet 34 model.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=19):\n",
    "        \"\"\"\n",
    "        Initialize resnet34 model and change last layer.\n",
    "        :param num_classes: int number of outputs.\n",
    "        \"\"\"\n",
    "        super(CustomResNet, self).__init__()\n",
    "        resnet = resnet34(weights=ResNet34_Weights.IMAGENET1K_V1)\n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "        in_features = resnet.fc.in_features\n",
    "        resnet.fc = nn.Linear(in_features, num_classes)\n",
    "        self.resnet = resnet\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "\n",
    "class CustomModel(pl.LightningModule):\n",
    "    def __init__(self, model, threshold=0.7, k=4):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.model = model\n",
    "        self.train_loss_mean = []\n",
    "        self.train_acc_mean = []\n",
    "        self.train_k_acc = []\n",
    "        self.val_loss_mean = []\n",
    "        self.val_acc_mean = []\n",
    "        self.val_k_acc = []\n",
    "        self.k = k\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def adversarial_loss(self, y_hat, y):\n",
    "        \"\"\"\n",
    "        Initialize loss function.\n",
    "        :param y_hat: prediction.\n",
    "        :param y: real values.\n",
    "        :return: loss function.\n",
    "        \"\"\"\n",
    "        loss_fn = LabelSmoothingBCEWithLogitsLossFlat()\n",
    "        return loss_fn(y_hat, y)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Step of the training loop.\n",
    "        :param batch: batch for training.\n",
    "        :param batch_idx: index of trained batch.\n",
    "        :return: loss calculated on this step.\n",
    "        \"\"\"\n",
    "        images, attributes = batch\n",
    "        outputs = self(images)\n",
    "        loss = self.adversarial_loss(outputs, attributes)\n",
    "        self.train_loss_mean.append(loss)\n",
    "        accuracy = self.calculate_accuracy(outputs, attributes)\n",
    "        k_acc = self.top_k_accuracy(outputs, attributes)\n",
    "        self.train_acc_mean.append(accuracy)\n",
    "        self.train_k_acc.append(k_acc)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Step of the validation loop.\n",
    "        :param batch: batch for validation.\n",
    "        :param batch_idx: index of validation batch.\n",
    "        :return: dictionary with validation loss and accuracy.\n",
    "        \"\"\"\n",
    "        images, attributes = batch\n",
    "        outputs = self(images)\n",
    "        loss = self.adversarial_loss(outputs, attributes)\n",
    "        self.val_loss_mean.append(loss)\n",
    "        accuracy = self.calculate_accuracy(outputs, attributes)\n",
    "        k_acc = self.top_k_accuracy(outputs, attributes)\n",
    "        self.val_acc_mean.append(accuracy)\n",
    "        self.val_k_acc.append(k_acc)\n",
    "        return {\"val_loss\": loss, \"val_accuracy\": accuracy, \"val_k_acc\": k_acc}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "        Initialize optimizer.\n",
    "        :return: optimizer.\n",
    "        \"\"\"\n",
    "        return torch.optim.Adam(self.model.parameters(), lr=0.001)\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        \"\"\"\n",
    "        Calculate mean values when validation epoch ends.\n",
    "        \"\"\"\n",
    "        loss = sum(self.val_loss_mean) / len(self.val_loss_mean)\n",
    "        self.val_loss_mean = []\n",
    "        acc = sum(self.val_acc_mean) / len(self.val_acc_mean)\n",
    "        self.val_acc_mean = []\n",
    "        k_acc = sum(self.val_k_acc) / len(self.val_k_acc)\n",
    "        self.val_k_acc = []\n",
    "        self.log(\"val epoch end loss\", loss, prog_bar=True)\n",
    "        self.log(\"val epoch end acc\", acc, prog_bar=True)\n",
    "        self.log(\"val epoch end k acc\", k_acc, prog_bar=True)\n",
    "\n",
    "    def calculate_accuracy(self, outputs, targets):\n",
    "        \"\"\"\n",
    "        Calculate the quality of the model.\n",
    "        :param outputs: model outputs.\n",
    "        :param targets: targets: real values.\n",
    "        :return: float value - accuracy.\n",
    "        \"\"\"\n",
    "        probs = F.softmax(outputs, dim=1)\n",
    "        binary_mask = (probs >= self.threshold).float()\n",
    "        accuracy = fbeta_score(binary_mask, targets, beta=2, average='samples')\n",
    "        return accuracy\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        \"\"\"\n",
    "        Calculate mean values when trining epoch ends.\n",
    "        \"\"\"\n",
    "        loss = sum(self.train_loss_mean) / len(self.train_loss_mean)\n",
    "        self.train_loss_mean = []\n",
    "        acc = sum(self.train_acc_mean) / len(self.train_acc_mean)\n",
    "        self.train_acc_mean = []\n",
    "        k_acc = sum(self.train_k_acc) / len(self.train_k_acc)\n",
    "        self.train_k_acc = []\n",
    "        self.log(\"train epoch end loss\", loss, prog_bar=True)\n",
    "        self.log(\"train epoch end acc\", acc, prog_bar=True)\n",
    "        self.log(\"train epoch end k acc\", k_acc, prog_bar=True)\n",
    "\n",
    "    def top_k_accuracy(self, outputs, targets):\n",
    "        \"\"\"\n",
    "        Calculate accuracy among k most probable classes.\n",
    "        :param outputs: model outputs.\n",
    "        :param targets: real values.\n",
    "        :return: float value - accuracy.\n",
    "        \"\"\"\n",
    "        topk_values, topk_indices = torch.topk(outputs, self.k, dim=1)\n",
    "        correct_count = 0\n",
    "        for i in range(topk_indices.size(0)):\n",
    "            for j in range(topk_indices.size(1)):\n",
    "                if targets[i, topk_indices[i, j]] == 1:\n",
    "                    correct_count += 1\n",
    "        accuracy = correct_count / (outputs.size(0) * self.k)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Cqe512LWAQP6"
   },
   "outputs": [],
   "source": [
    "class Clothes:\n",
    "    attributes = ['floral', 'graphic', 'striped', 'embroidered', 'solid', 'lattice',\n",
    "                  'long_sleeve', 'short_sleeve', 'sleeveless', 'maxi_length',\n",
    "                  'mini_length', 'crew_neckline', 'v_neckline', 'square_neckline',\n",
    "                  'no_neckline', 'denim', 'tight', 'loose', 'conventional']\n",
    "    not_acceptable_attributes = ['sleeveless', 'mini_length', 'denim', 'tight', 'loose']\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize transforms.\n",
    "        \"\"\"\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def compute_image_sharpness(self, image):\n",
    "        \"\"\"\n",
    "        Calculate sharpness of one image.\n",
    "        :param image: image to process.\n",
    "        :return: float value - sharpness of image.\n",
    "        \"\"\"\n",
    "        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        return cv2.Laplacian(gray_image, cv2.CV_64F).var()\n",
    "\n",
    "    def choose_sharpest_image(self, images):\n",
    "        \"\"\"\n",
    "        Choose sharpest image for future assessing.\n",
    "        :param images: frames for choosing.\n",
    "        :return: sharpest image.\n",
    "        \"\"\"\n",
    "        sharpest_image = None\n",
    "        max_sharpness = 0\n",
    "\n",
    "        for image in images:\n",
    "            sharpness = self.compute_image_sharpness(image)\n",
    "            if sharpness > max_sharpness:\n",
    "                max_sharpness = sharpness\n",
    "                sharpest_image = image\n",
    "\n",
    "        return sharpest_image\n",
    "\n",
    "    def transform_image(self, image):\n",
    "        \"\"\"\n",
    "        Transform image into model input.\n",
    "        :param image: image for processing.\n",
    "        :return: tensor - transformed image.\n",
    "        \"\"\"\n",
    "        mp_pose = mp.solutions.pose\n",
    "        pose = mp_pose.Pose()\n",
    "\n",
    "        image_h, image_w, _ = image.shape\n",
    "        results = pose.process(image)\n",
    "\n",
    "        if results.pose_landmarks:\n",
    "            # Identify bound box.\n",
    "            x_min, y_min, x_max, y_max = image_w, image_h, 0, 0\n",
    "            for landmark in results.pose_landmarks.landmark:\n",
    "                x, y = int(landmark.x * image_w), int(landmark.y * image_h)\n",
    "                x_min = max(0, min(x_min, x))\n",
    "                y_min = max(0, min(y_min, y))\n",
    "                x_max = min(image_w - 1, max(x_max, x))\n",
    "                y_max = min(image_h - 1, max(y_max, y))\n",
    "            image = image[y_min:y_max, x_min:x_max]\n",
    "\n",
    "        pose.close()\n",
    "        pil_image = Image.fromarray(image)\n",
    "        image = self.transform(pil_image)\n",
    "        return image\n",
    "\n",
    "    def check_arrays(self, arr1, arr2):\n",
    "        \"\"\"\n",
    "        Check presence of first array elements in second array.\n",
    "        :param arr1: array for checking elements.\n",
    "        :param arr2: second array for processing.\n",
    "        :return: bool value if none of elements in first array is in the second.\n",
    "        \"\"\"\n",
    "        for elem in arr1:\n",
    "            if elem in arr2:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def assess_appearance(self, frames):\n",
    "        \"\"\"\n",
    "        Assess clothes attributes.\n",
    "        :param frames: frames for choosing best frame for processing.\n",
    "        :return: bool value if clothes is acceptable.\n",
    "        \"\"\"\n",
    "        model = CustomResNet()\n",
    "        custom_model = CustomModel(model)\n",
    "\n",
    "        path = \"./saved_model_modified.pth\"\n",
    "\n",
    "        custom_model.model.load_state_dict(torch.load(path))\n",
    "        image = self.choose_sharpest_image(frames)\n",
    "        image = self.transform_image(image)\n",
    "        image = image.unsqueeze(0)\n",
    "        custom_model.eval()\n",
    "        output = custom_model(image)\n",
    "        pred = F.softmax(output, dim=1)\n",
    "        topk_values, topk_indices = torch.topk(pred, 3, dim=1)\n",
    "        captions = []\n",
    "        for i in range(topk_indices.size(0)):\n",
    "            for j in range(topk_indices.size(1)):\n",
    "                captions.append(Clothes.attributes[topk_indices[i, j]])\n",
    "        return self.check_arrays(captions, Clothes.not_acceptable_attributes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "iciHQjvCAY7K"
   },
   "outputs": [],
   "source": [
    "class DrawResults:\n",
    "    def __init__(self, path, dist=10, good_color=(0,128,0), bad_color=(60,20,220)):\n",
    "        self.video_path = path\n",
    "        self.dist = dist\n",
    "        self.right_color = good_color\n",
    "        self.not_right_color = bad_color\n",
    "\n",
    "    def draw_frames(self, frame, text, color_flag):\n",
    "        \"\"\"\n",
    "        Draw results on video frames.\n",
    "        :param frames: frames for processing.\n",
    "        :return: processed frames.\n",
    "        \"\"\"\n",
    "        font = cv2.FONT_HERSHEY_COMPLEX\n",
    "        x = 20\n",
    "        y = 30\n",
    "        font_scale = 0.5\n",
    "        thickness = 1\n",
    "        count = 1\n",
    "        for i in range(len(text)):\n",
    "            if color_flag[i]:\n",
    "                color = self.right_color\n",
    "            else:\n",
    "                color = self.not_right_color\n",
    "            if text[i] is not None:\n",
    "                frame = cv2.putText(frame, text[i], (x, y * count), font, font_scale, color, thickness, cv2.LINE_AA)\n",
    "                count += 1\n",
    "        return frame\n",
    "\n",
    "    def draw_angle(self, frame, length, color):\n",
    "        \"\"\"\n",
    "        Draw lines for correct angle.\n",
    "        :param frame: image for drawing.\n",
    "        :param length: length of speaker's bound box.\n",
    "        :param color: red if angle is incorrect, green otherwise.\n",
    "        :return: new frame with angle lines.\n",
    "        \"\"\"\n",
    "        image_orig = frame.copy()\n",
    "        height, width = frame.shape[:2]\n",
    "        center_x = width // 2\n",
    "        line_length = length // 2\n",
    "        line_thickness = 5\n",
    "        line_offset_top = height // 3 + int(0.15 * height)\n",
    "        line_offset_bottom = height // 3 - int(0.15 * height)\n",
    "        font_color = self.right_color\n",
    "        if not color:\n",
    "            font_color = self.not_right_color\n",
    "        cv2.line(frame, (center_x - line_length, height), (center_x - line_length, 0), font_color, line_thickness)\n",
    "        cv2.line(frame, (center_x + line_length, height), (center_x + line_length, 0), font_color, line_thickness)\n",
    "        cv2.line(frame, (center_x - line_length, line_offset_top), (center_x + line_length, line_offset_top),\n",
    "                 font_color, line_thickness)\n",
    "        cv2.line(frame, (center_x - line_length, line_offset_bottom), (center_x + line_length, line_offset_bottom),\n",
    "                 font_color, line_thickness)\n",
    "        font = cv2.FONT_HERSHEY_COMPLEX\n",
    "        bottom_left_corner_text = (center_x - line_length, line_offset_top - 20)\n",
    "        font_scale = 0.5\n",
    "        line_type = 1\n",
    "        cv2.putText(frame, 'Recommended eye level', bottom_left_corner_text, font, font_scale, font_color,\n",
    "                    line_type)\n",
    "        image_out = cv2.addWeighted(frame, 0.3, image_orig, 0.7, 0.0)\n",
    "        return image_out\n",
    "\n",
    "    def draw(self, output_path, text, colors, angle, angle_color):\n",
    "        cap = cv2.VideoCapture(self.video_path)\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
    "        try:\n",
    "            segment_duration = self.dist\n",
    "            segment_frame_count = math.ceil(fps * segment_duration)\n",
    "            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            i = 0\n",
    "            while True:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                ind = i // segment_frame_count\n",
    "                text_elements = [row[ind] for row in text]\n",
    "                colors_elements = [row[ind] for row in colors]\n",
    "                frame = self.draw_frames(frame, text_elements, colors_elements)\n",
    "                if len(angle) > 0 and angle[ind] is not None:\n",
    "                    color = True\n",
    "                    if i % segment_frame_count in angle_color[ind]:\n",
    "                        color = False\n",
    "                    frame = self.draw_angle(frame, angle[ind], color)\n",
    "                out.write(frame)\n",
    "                i += 1\n",
    "        except Exception as e:\n",
    "            print(e.args)\n",
    "        finally:\n",
    "            cap.release()\n",
    "            out.release()\n",
    "            cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "2dSwx3owAafI"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1745696774.462886 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1745696774.468036 6551973 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745696774.473977 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n"
     ]
    }
   ],
   "source": [
    "class VideoEmotions:\n",
    "    face_detection = mp.solutions.face_detection.FaceDetection(min_detection_confidence=0.3)\n",
    "    model_name = 'enet_b0_8_best_afew'\n",
    "    face_mesh = mp.solutions.face_mesh.FaceMesh()\n",
    "\n",
    "    def __init__(self, device='cpu', model='EmotiEffLib', engine=\"onnx\"):\n",
    "        \"\"\"\n",
    "        Initialize model and device.\n",
    "        :param device: cpu or gpu.\n",
    "        :param model: HSEmotion or deepFace.\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.model = model\n",
    "        if model == 'EmotiEffLib':\n",
    "            self.predictor = EmotiEffLibRecognizer(engine=engine, model_name=VideoEmotions.model_name, device=device)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_main_face(frame):\n",
    "        \"\"\"\n",
    "        Crop image to get main face on the frame.\n",
    "        @return: cropped image.\n",
    "        \"\"\"\n",
    "        results = VideoEmotions.face_detection.process(frame)\n",
    "        main_face = None\n",
    "        max_score = 0\n",
    "        if results.detections is not None:\n",
    "            for detection in results.detections:\n",
    "                if detection.score[0] > max_score:\n",
    "                    main_face = detection\n",
    "                    max_score = detection.score[0]\n",
    "            if main_face is not None:\n",
    "                bbox = main_face.location_data.relative_bounding_box\n",
    "                image_height, image_width, _ = frame.shape\n",
    "                x, y, w, h = int(bbox.xmin * image_width), int(bbox.ymin * image_height), \\\n",
    "                    int(bbox.width * image_width), int(bbox.height * image_height)\n",
    "                main_face = frame[y:y + h, x:x + w]\n",
    "        return main_face\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_emotion_percentage(emotion_list):\n",
    "        \"\"\"\n",
    "        Calculate percentage of each element in the list.\n",
    "        :param emotion_list: list for calculation.\n",
    "        :return: dictionary with percentages of each element.\n",
    "        \"\"\"\n",
    "        total_frames = len(emotion_list)\n",
    "        emotion_percentage = {}\n",
    "        for emotion in emotion_list:\n",
    "            if emotion in emotion_percentage.keys():\n",
    "                emotion_percentage[emotion] += 1\n",
    "            else:\n",
    "                emotion_percentage[emotion] = 1\n",
    "        for emotion in emotion_percentage.keys():\n",
    "            emotion_percentage[emotion] = (emotion_percentage[emotion] / total_frames) * 100\n",
    "        return emotion_percentage\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_emotion_change_frequency(emotion_list):\n",
    "        \"\"\"\n",
    "        calculate the percentage of changing emotions between two seconds.\n",
    "        :param emotion_list: list to calculate changes in it.\n",
    "        :return: frequency of changing emotions.\n",
    "        \"\"\"\n",
    "        total_frames = len(emotion_list)\n",
    "        emotion_changes = 0\n",
    "        for i in range(1, total_frames):\n",
    "            if emotion_list[i] != emotion_list[i - 1]:\n",
    "                emotion_changes += 1\n",
    "        emotion_change_frequency = emotion_changes / total_frames\n",
    "        return emotion_change_frequency\n",
    "\n",
    "    def process_frames(self, frames):\n",
    "        \"\"\"\n",
    "        Predict emotions on each frame.\n",
    "        :param frames: frames for processing.\n",
    "        :return: main emotions and probabilities for each frame.\n",
    "        \"\"\"\n",
    "        imgs = frames\n",
    "        faces = list(map(VideoEmotions.get_main_face, imgs))\n",
    "        emotions, scores = [], []\n",
    "        for face in faces:\n",
    "            if face is not None:\n",
    "                try:\n",
    "                    emotion, score = self.predictor.predict_emotions(face, logits=False)\n",
    "                    emotions.append(emotion)\n",
    "                    scores.append(score)\n",
    "                except Exception:\n",
    "                    continue\n",
    "        return emotions, scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "P__hf405Ab8R"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1745696774.477917 6551982 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696774.487823 6551985 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "class GazeDirection:\n",
    "    LEFT_EYE = [362, 382, 381, 380, 374, 373, 390, 249, 263, 466, 388, 387, 386, 385, 384, 398, 286, 258, 257, 259, 260]\n",
    "    RIGHT_IRIS = [468, 470, 469, 472, 471]\n",
    "    RIGHT_EYE = [33, 7, 163, 144, 145, 153, 154, 155, 133, 173, 157, 158, 159, 160, 161, 246, 30, 29, 28, 27, 56]\n",
    "    LEFT_IRIS = [473, 475, 474, 477, 476]\n",
    "\n",
    "    def __init__(self, threshold=0.1):\n",
    "        \"\"\"\n",
    "        Initialize prediction model and threshold.\n",
    "        :param threshold: float value - acceptable displacement of iris.\n",
    "        \"\"\"\n",
    "\n",
    "        path = \"./face_landmarker_v2_with_blendshapes.task\"\n",
    "\n",
    "        model_file = open(path, \"rb\")\n",
    "        model_data = model_file.read()\n",
    "        model_file.close()\n",
    "        base_options = python.BaseOptions(model_asset_buffer=model_data)\n",
    "        options = vision.FaceLandmarkerOptions(base_options=base_options,\n",
    "                                               output_face_blendshapes=True,\n",
    "                                               output_facial_transformation_matrixes=True,\n",
    "                                               num_faces=1)\n",
    "        self.detector = vision.FaceLandmarker.create_from_options(options)\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def count_displacement(self, eye_coords, iris_coords):\n",
    "        \"\"\"\n",
    "        Calculate the position of iris in percent relatively center.\n",
    "        :param eye_coords: all coordinates of eye.\n",
    "        :param iris_coords: all coordinates of iris.\n",
    "        :return: percent of x and y axis - position of an iris.\n",
    "        \"\"\"\n",
    "        max_x = (max(eye_coords, key=lambda item: item[0]))[0]\n",
    "        min_x = (min(eye_coords, key=lambda item: item[0]))[0]\n",
    "        max_y = (max(eye_coords, key=lambda item: item[1]))[1]\n",
    "        min_y = (min(eye_coords, key=lambda item: item[1]))[1]\n",
    "        width = max_x - min_x\n",
    "        height = max_y - min_y\n",
    "        iris_x = iris_coords[0][0]\n",
    "        iris_y = iris_coords[0][1]\n",
    "        percent_x = (2 * iris_x - width - 2 * min_x) / width\n",
    "        percent_y = (2 * iris_y - height - 2 * min_y) / height\n",
    "        return percent_x, percent_y\n",
    "\n",
    "    def process_gaze(self, right_x, right_y, left_x, left_y):\n",
    "        \"\"\"\n",
    "        Asses gaze.\n",
    "        :param right_x: x position of right iris.\n",
    "        :param right_y: y position of right iris.\n",
    "        :param left_x: x position of left iris.\n",
    "        :param left_y: y position of left iris.\n",
    "        :return: string value - gaze direction.\n",
    "        \"\"\"\n",
    "        x = (right_x + left_x) / 2\n",
    "        y = (right_y + left_y) / 2\n",
    "        if y > 0.45:\n",
    "            result = \"down \"\n",
    "        elif y < 0.2:\n",
    "            result = \"up \"\n",
    "        else:\n",
    "            result = \"\"\n",
    "\n",
    "        if abs(x) > self.threshold and x > 0:\n",
    "            result += \"right\"\n",
    "        elif abs(x) > self.threshold and x < 0:\n",
    "            result += \"left\"\n",
    "        else:\n",
    "            result += \"center\"\n",
    "        return result\n",
    "\n",
    "    def landmarks_detection(self, img_width, img_height, face_landmarks, ind):\n",
    "        \"\"\"\n",
    "        Transform coordinates into pixels of image.\n",
    "        :param img_width: width of an image.\n",
    "        :param img_height: height of an image.\n",
    "        :param face_landmarks: not transformed landmarks.\n",
    "        :param ind: indexes of required points.\n",
    "        :return: transformed coordinates.\n",
    "        \"\"\"\n",
    "        mesh_coord = [(int(face_landmarks[i].x * img_width), int(face_landmarks[i].y * img_height)) for i in ind]\n",
    "        return mesh_coord\n",
    "\n",
    "    def gaze_detection(self, frames):\n",
    "        \"\"\"\n",
    "        Calculate direction of eyes on each frame.\n",
    "        :param frames: frames for processing.\n",
    "        :return: list with string results for all frames.\n",
    "        \"\"\"\n",
    "        result_list = []\n",
    "        for frame in frames:\n",
    "            image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame)\n",
    "            results = self.detector.detect(image)\n",
    "            frame_width = frame.shape[0]\n",
    "            frame_height = frame.shape[1]\n",
    "            try:\n",
    "                face_landmarks = results.face_landmarks[0]\n",
    "                left_iris_coords = self.landmarks_detection(frame_width, frame_height, face_landmarks, GazeDirection.LEFT_IRIS)\n",
    "                right_iris_coords = self.landmarks_detection(frame_width, frame_height, face_landmarks, GazeDirection.RIGHT_IRIS)\n",
    "                left_eye_coords = self.landmarks_detection(frame_width, frame_height, face_landmarks, GazeDirection.LEFT_EYE)\n",
    "                right_eye_coords = self.landmarks_detection(frame_width, frame_height, face_landmarks, GazeDirection.RIGHT_EYE)\n",
    "                right_x, right_y = self.count_displacement(right_eye_coords, right_iris_coords)\n",
    "                left_x, left_y = self.count_displacement(left_eye_coords, left_iris_coords)\n",
    "                res = self.process_gaze(right_x, right_y, left_x, left_y)\n",
    "                result_list.append(res)\n",
    "            except Exception as ex:\n",
    "                continue\n",
    "        return result_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "nwmfv4dlAdUD"
   },
   "outputs": [],
   "source": [
    "class Gestures:\n",
    "    body_angles = [[16, 14, 12], [14, 12, 11], [15, 13, 11], [12, 11, 13],\n",
    "                   [21, 15, 19], [19, 15, 17], [22, 16, 20], [20, 16, 18],\n",
    "                   [18, 20, 16, 14], [17, 19, 15, 13], [11, 0, 12]]\n",
    "\n",
    "    def __init__(self):\n",
    "        self.body_res = {16: {'name': 'right elbow', 'res': []}, 14: {'name': 'right shoulder', 'res': []},\n",
    "                         15: {'name': 'left elbow', 'res': []},\n",
    "                         12: {'name': 'left shoulder', 'res': []},\n",
    "                         21: {'name': 'left thumb', 'res': []},\n",
    "                         19: {'name': 'left pinky', 'res': []}, 22: {'name': 'right thumb', 'res': []},\n",
    "                         20: {'name': 'right pinky', 'res': []},\n",
    "                         18: {'name': 'right wrist', 'res': []}, 17: {'name': 'left wrist', 'res': []},\n",
    "                         11: {'name': 'head', 'res': []}}\n",
    "\n",
    "    def get_vector_between_points(self, first_point, second_point):\n",
    "        \"\"\"\n",
    "        Calculate vector between two points in 2d.\n",
    "        :param first_point: list or array with 2 elements (x and y) - first point to calculate vector.\n",
    "        :param second_point: list or array with 2 elements (x and y) - second point to calculate vector.\n",
    "        :return: list wit x and y of calculated vector.\n",
    "        \"\"\"\n",
    "        x1, y1 = first_point[0], first_point[1]\n",
    "        x2, y2 = second_point[0], second_point[1]\n",
    "        vector = np.array([x2, y2]) - np.array([x1, y1])\n",
    "        return vector\n",
    "\n",
    "    def angle_between_vectors(self, v1, v2):\n",
    "        \"\"\"\n",
    "        Calculate angle in degrees between given vectors.\n",
    "        :param v1: list or array with 2 elements (x and y) - first vector.\n",
    "        :param v2: list or array with 2 elements (x and y) - second vector.\n",
    "        :return: float value [0:360] - angle between v1 and v2.\n",
    "        \"\"\"\n",
    "        dot_product = np.dot(v1, v2)\n",
    "        norm_v1 = np.linalg.norm(v1)\n",
    "        norm_v2 = np.linalg.norm(v2)\n",
    "        cos_theta = dot_product / (norm_v1 * norm_v2)\n",
    "        angle_rad = np.arccos(np.clip(cos_theta, -1.0, 1.0))\n",
    "        angle_deg = np.degrees(angle_rad)\n",
    "\n",
    "        # Check the angle of the sign and adjust it in the range from 0 to 360 degrees.\n",
    "        if np.cross(v1, v2) < 0:\n",
    "            angle_deg = 360 - angle_deg\n",
    "\n",
    "        return angle_deg\n",
    "\n",
    "    def min_angle_difference(self, angle1, angle2):\n",
    "        \"\"\"\n",
    "        Get min differance between two angles.\n",
    "        :param angle1: float value [0:360] - first value in degrees.\n",
    "        :param angle2: float value [0:360] - second value in degrees.\n",
    "        :return: float value [0:360] - min angle between two angles in closed circle.\n",
    "        \"\"\"\n",
    "        diff1 = abs(angle1 - angle2)\n",
    "        diff2 = 360 - diff1\n",
    "        return min(diff1, diff2)\n",
    "\n",
    "    def point_between(self, point1, point2):\n",
    "        \"\"\"\n",
    "        Calculate point between 2 points in 2d.\n",
    "        :param point1: landmark with x and y attributes - first point.\n",
    "        :param point2: landmark with x and y attributes - second point.\n",
    "        :return: list with x and y of point between 2 given points.\n",
    "        \"\"\"\n",
    "        return [(point1.x + point2.x) / 2, (point1.y + point2.y) / 2]\n",
    "\n",
    "    def calculate_angles(self, landmarks, mean_angle):\n",
    "        \"\"\"\n",
    "        calculate the displacement of the joints between frames.\n",
    "        :param landmarks: coordinates of the main joints.\n",
    "        :param mean_angle: dictionary for calculation results.\n",
    "        :return: dictionary with results.\n",
    "        \"\"\"\n",
    "        for angles in Gestures.body_angles:\n",
    "            if all(landmarks[angle].visibility >= 0.5 for angle in angles):\n",
    "                point_second = [landmarks[angles[-1]].x, landmarks[angles[-1]].y]\n",
    "                point_mid = [landmarks[angles[-2]].x, landmarks[angles[-2]].y]\n",
    "                if len(angles) > 3:\n",
    "                    point_first = self.point_between(landmarks[angles[0]], landmarks[angles[1]])\n",
    "                else:\n",
    "                    point_first = [landmarks[angles[0]].x, landmarks[angles[0]].y]\n",
    "                v1 = self.get_vector_between_points(point_first, point_mid)\n",
    "                v2 = self.get_vector_between_points(point_mid, point_second)\n",
    "                angle = self.angle_between_vectors(v1, v2)\n",
    "                if mean_angle[angles[0]]['prev'] is not None:\n",
    "                    mean_angle[angles[0]]['res'] += self.min_angle_difference(angle, mean_angle[angles[0]]['prev'])\n",
    "                    mean_angle[angles[0]]['count'] += 1\n",
    "                else:\n",
    "                    mean_angle[angles[0]]['prev'] = angle\n",
    "            else:\n",
    "                mean_angle[angles[0]]['prev'] = None\n",
    "        return mean_angle\n",
    "\n",
    "    def process_velocity(self, frames):\n",
    "        \"\"\"\n",
    "        Count angle displacement for all frames.\n",
    "        :param frames: frames to process.\n",
    "        :return: dictionary with results for each joint.\n",
    "        \"\"\"\n",
    "        mp_pose = mp.solutions.pose\n",
    "        with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "            mean_angle = {}\n",
    "            for angle in Gestures.body_angles:\n",
    "                mean_angle[angle[0]] = {}\n",
    "                mean_angle[angle[0]]['prev'] = None\n",
    "                mean_angle[angle[0]]['count'] = 0\n",
    "                mean_angle[angle[0]]['res'] = 0\n",
    "            for image in frames:\n",
    "                results = pose.process(image)\n",
    "                try:\n",
    "                    landmarks = results.pose_landmarks.landmark\n",
    "                    mean_angle = self.calculate_angles(landmarks, mean_angle)\n",
    "                except Exception as ex:\n",
    "                    continue\n",
    "            for angle in Gestures.body_angles:\n",
    "                if mean_angle[angle[0]]['count'] > 0:\n",
    "                    result = mean_angle[angle[0]]['res'] / mean_angle[angle[0]]['count']\n",
    "                    self.body_res[angle[0]]['res'].append(round(result, 2))\n",
    "                else:\n",
    "                    self.body_res[angle[0]]['res'].append(0)\n",
    "\n",
    "    def get_result(self):\n",
    "        \"\"\"\n",
    "        Get result angles for body parts.\n",
    "        :return: dictionary with body parts as keys and angles as values.\n",
    "        \"\"\"\n",
    "        return {value['name']: value['res'] for key, value in self.body_res.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "8sNmDDrMAe9Z"
   },
   "outputs": [],
   "source": [
    "class Perspective:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize model for detection.\n",
    "        \"\"\"\n",
    "        self.detector = PoseDetector(staticMode=False,\n",
    "                                modelComplexity=1,\n",
    "                                smoothLandmarks=True,\n",
    "                                enableSegmentation=False,\n",
    "                                smoothSegmentation=True,\n",
    "                                detectionCon=0.5,\n",
    "                                trackCon=0.5)\n",
    "\n",
    "    def point_between(self, point1, point2):\n",
    "        \"\"\"\n",
    "        Calculate point between 2 points in 2d.\n",
    "        :param point1: list with x and y of first point.\n",
    "        :param point2: list with x and y of second point.\n",
    "        :return: list with x and y of mid point.\n",
    "        \"\"\"\n",
    "        return [(point1[0] + point2[0]) / 2, (point1[1] + point2[1]) / 2]\n",
    "\n",
    "    def count_brightness(self, frames):\n",
    "        \"\"\"\n",
    "        Asses lightning on frames.\n",
    "        :param frames: list with frames to process.\n",
    "        :return: string value - lightning.\n",
    "        \"\"\"\n",
    "        dark = 0\n",
    "        optimal = 0\n",
    "        bright = 0\n",
    "        for frame in frames:\n",
    "            gray_image = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "            mean_brightness = np.mean(gray_image)\n",
    "            if mean_brightness < 100:\n",
    "                dark += 1\n",
    "            elif mean_brightness > 200:\n",
    "                bright += 1\n",
    "            else:\n",
    "                optimal += 1\n",
    "        dark /= len(frames)\n",
    "        optimal /= len(frames)\n",
    "        bright /= len(frames)\n",
    "        if dark >= optimal and dark >= bright:\n",
    "            return 0\n",
    "        elif bright >= dark and bright >= optimal:\n",
    "            return 2\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    def check_correct_pose(self, bounding_box, eye_coords, image_width, image_height):\n",
    "        \"\"\"\n",
    "        Check if speaker in a right position.\n",
    "        :param bounding_box: coordinates of the speakers bound box.\n",
    "        :param eye_coords: eyes coordinates.\n",
    "        :param image_width: width of an image.\n",
    "        :param image_height: height of an image.\n",
    "        :return: 1 or 0 - if position is correct.\n",
    "        \"\"\"\n",
    "        x_center = image_width // 2\n",
    "        y_third_line = image_height // 3\n",
    "        x, y, x_len, y_len = bounding_box[\"bbox\"]\n",
    "        if abs(x + x_len / 2 - x_center) > 0.2 * x_center:\n",
    "            return False\n",
    "\n",
    "        # Check eye position according rule of the third.\n",
    "        eye_x, eye_y = eye_coords\n",
    "        if eye_y < y_third_line - 0.15 * image_height or eye_y > y_third_line + 0.15 * image_height:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def count_angle(self, frames):\n",
    "        \"\"\"\n",
    "        Count percent of incorrect frames.\n",
    "        :param frames: frames to process.\n",
    "        :return: percent of incorrect frames.\n",
    "        \"\"\"\n",
    "        incorrect_pose = 0\n",
    "        bbox_length = 0\n",
    "        inc_index = []\n",
    "        ind = 0\n",
    "        for frame in frames:\n",
    "            img = self.detector.findPose(frame, draw=False)\n",
    "            lm_list, bbox_info = self.detector.findPosition(img, draw=False, bboxWithHands=False)\n",
    "            if len(lm_list) == 0:\n",
    "                incorrect_pose += 1\n",
    "                continue\n",
    "            right_coords = [lm_list[5][0], lm_list[5][1]]\n",
    "            left_coords = [lm_list[2][0], lm_list[2][1]]\n",
    "            height, width = frame.shape[:2]\n",
    "            if not self.check_correct_pose(bbox_info, self.point_between(right_coords, left_coords), width, height):\n",
    "                inc_index.append(ind)\n",
    "                incorrect_pose += 1\n",
    "            ind += 1\n",
    "            length = bbox_info['bbox'][2] - bbox_info['bbox'][0]\n",
    "            if length > bbox_length:\n",
    "                bbox_length = length\n",
    "        return incorrect_pose / len(frames), bbox_length, inc_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FacialAttributeRecognizer:\n",
    "    #supported values of model_name: age_gender_ethnicity_lagenda_mbf_ft, age_gender_ethnicity_lagenda_enet0_ft\n",
    "    def __init__(self, model_name='age_gender_ethnicity_lagenda_mbf_ft'):\n",
    "        if 'mbf' in model_name:\n",
    "            self.mean=[0.5, 0.5, 0.5]\n",
    "            self.std=[0.5, 0.5, 0.5]\n",
    "            self.img_size=112\n",
    "        else:\n",
    "            self.mean=[0.485, 0.456, 0.406]\n",
    "            self.std=[0.229, 0.224, 0.225]\n",
    "            self.img_size=224\n",
    "        self.num_classes=96\n",
    "        self.ort_session = ort.InferenceSession('./age_gender/models/' + model_name + '.onnx', providers=['CPUExecutionProvider'])\n",
    "    \n",
    "    def preprocess(self,img):\n",
    "        x=cv2.resize(img,(self.img_size,self.img_size))/255\n",
    "        for i in range(3):\n",
    "            x[..., i] = (x[..., i]-self.mean[i])/self.std[i]\n",
    "        return x.transpose(2, 0, 1).astype(\"float32\")[np.newaxis,...]\n",
    "\n",
    "    @staticmethod\n",
    "    def expected_age(age_probabs):        \n",
    "        indices=age_probabs.argsort()[::-1]#[:2]\n",
    "        norm_preds=age_probabs[indices]/np.sum(age_probabs[indices])\n",
    "\n",
    "        res_age=0\n",
    "        for age,probab in zip(indices,norm_preds):\n",
    "            res_age+=age*probab\n",
    "        return res_age\n",
    "\n",
    "    @staticmethod\n",
    "    def get_ethnicity_descriptions():\n",
    "        return ['','White', 'Black', 'Asian', 'Indian', 'Latino or Middle Eastern']\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_ethnicity(ethnicity_preds):\n",
    "        if ethnicity_preds is not None:\n",
    "            ethnicity_ind=np.argmax(ethnicity_preds)+1\n",
    "        else:\n",
    "            ethnicity_ind=0\n",
    "        return FacialAttributeRecognizer.get_ethnicity_descriptions()[ethnicity_ind]\n",
    "    \n",
    "    def get_attribute_probabs(self,face_img):\n",
    "        scores=self.ort_session.run(None,{\"input\": self.preprocess(face_img)})[0][0]\n",
    "        #print(scores)\n",
    "        age_probabs=np.exp(scores[:self.num_classes])\n",
    "        age_probabs=age_probabs/age_probabs.sum()\n",
    "\n",
    "        gender_probabs=np.exp(scores[self.num_classes:self.num_classes+2])\n",
    "        gender_probabs=gender_probabs/gender_probabs.sum()\n",
    "        #isMale=gender_probabs[0]>gender_probabs[1]\n",
    "        \n",
    "        ethnicity_probabs=None\n",
    "        if len(scores)>self.num_classes+2:\n",
    "            ethnicity_probabs=np.exp(scores[self.num_classes+2:])\n",
    "            ethnicity_probabs=ethnicity_probabs/ethnicity_probabs.sum()\n",
    "        return age_probabs,gender_probabs[0],ethnicity_probabs\n",
    "\n",
    "    def predict_attributes(self,face_img, estimage_age=False):\n",
    "        age_probabs,male_probab,ethnicity_probabs=self.get_attribute_probabs(face_img)\n",
    "        if estimage_age:\n",
    "            age_pred=self.expected_age(age_probabs)\n",
    "        else:\n",
    "            age_pred=age_probabs.argmax()\n",
    "        return age_pred,male_probab>0.5,self.get_ethnicity(ethnicity_probabs)\n",
    "\n",
    "    def process_frames(self, frames):\n",
    "        \"\"\"\n",
    "        Extract facial attributes on each frame.\n",
    "        :param frames: frames for processing.\n",
    "        :return: main age, gender, ethniity and their probabilities for each frame.\n",
    "        \"\"\"\n",
    "        imgs = frames\n",
    "        faces = list(map(VideoEmotions.get_main_face, imgs))\n",
    "        \n",
    "        age_labels, gender_labels, ethnicity_labels, age_scores, gender_scores, ethnicity_scores = [], [], [], [], [], []\n",
    "        for face in faces:\n",
    "            if face is not None:\n",
    "                try:\n",
    "                    age_score, gender_score, ethnicity_score = self.get_attribute_probabs(face)\n",
    "                    age = self.expected_age(age_score)\n",
    "                    gender = \"male\" if gender_score > 0.5 else \"female\"\n",
    "                    ethnicity = self.get_ethnicity(ethnicity_score)\n",
    "                    age_labels.append(age)\n",
    "                    age_scores.append(age_score)\n",
    "                    gender_labels.append(gender)\n",
    "                    gender_scores.append(gender_score)\n",
    "                    ethnicity_labels.append(ethnicity)\n",
    "                    ethnicity_scores.append(ethnicity_score)\n",
    "                except Exception:\n",
    "                    continue\n",
    "        return age_labels, gender_labels, ethnicity_labels, age_scores, gender_scores, ethnicity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "pwHDIcaMAhFl"
   },
   "outputs": [],
   "source": [
    "class VideoSubsystem:\n",
    "    acceptable_velocity = {'right elbow': [5, 50], 'left elbow': [5, 50], 'left shoulder': [2, 25],\n",
    "                           'right shoulder': [2, 25], 'left thumb': [0, 30], 'left pinky': [3, 40],\n",
    "                           'right thumb': [0, 30], 'right pinky': [3, 40], 'right wrist': [3, 40],\n",
    "                           'left wrist': [3, 40], 'head': [0, 12]}\n",
    "\n",
    "    def __init__(self, path, inappropriate_emotions, emotions=True, gesticulation=True, angle=True, gaze=True, clothes=True, facial_attrs=True,\n",
    "                 device='cpu', dist=5, acceptable_angle=0.6):\n",
    "        self.fps = None\n",
    "        self.inappropriate_emotions = inappropriate_emotions\n",
    "        self.device = device\n",
    "        self.video_path = path\n",
    "        self.emotions = emotions\n",
    "        self.gesticulation = gesticulation\n",
    "        self.angle = angle\n",
    "        self.gaze = gaze\n",
    "        self.clothes = clothes\n",
    "        self.facial_attrs = facial_attrs\n",
    "        self.dist = dist\n",
    "        self.acceptable_angle = acceptable_angle\n",
    "\n",
    "        path = \"./model_first.joblib\"\n",
    "        self.emotion_model = load(path)\n",
    "        self.emotion_list = []\n",
    "        self.emotion_inappropriate_percentage = []\n",
    "        self.gesture_list = []\n",
    "        self.angle_list = []\n",
    "        self.gaze_list = []\n",
    "        self.lightning = []\n",
    "        self.angle_len = []\n",
    "        self.inc_ind = []\n",
    "        self.clothes_estimation = None\n",
    "        self.age_list = []\n",
    "        self.gender_list = []\n",
    "        self.ethnicity_list = []\n",
    "\n",
    "    def get_emotions(self):\n",
    "        return self.emotion_list\n",
    "\n",
    "    def get_ages(self):\n",
    "        return self.age_list\n",
    "\n",
    "    def get_genders(self):\n",
    "        return self.gender_list\n",
    "\n",
    "    def get_ethnicities(self):\n",
    "        return self.ethnicity_list\n",
    "\n",
    "    def get_gestures(self):\n",
    "        return self.gesture_list\n",
    "\n",
    "    def get_angle(self):\n",
    "        return self.angle_list\n",
    "\n",
    "    def get_gaze(self):\n",
    "        return self.gaze_list\n",
    "\n",
    "    def get_lightning(self):\n",
    "        return self.lightning\n",
    "\n",
    "    def get_angle_len(self):\n",
    "        return self.angle_len\n",
    "\n",
    "    def get_clothes_estimation(self):\n",
    "        return self.clothes_estimation\n",
    "\n",
    "    def get_incorrect_angle_ind(self):\n",
    "        return self.inc_ind\n",
    "\n",
    "    def get_inappropriate_emotion_percentage(self):\n",
    "        return self.emotion_inappropriate_percentage\n",
    "    @staticmethod\n",
    "    def get_subarray(array, subset, ind):\n",
    "        \"\"\"\n",
    "        Get subarray.\n",
    "        :param array: array to get subarray from it.\n",
    "        :param subset: number of elements in subarray.\n",
    "        :param ind: index of array from which subarray starts.\n",
    "        :return: subarray.\n",
    "        \"\"\"\n",
    "        last_ind = min(ind + subset, len(array))\n",
    "        return array[ind:last_ind]\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_percentage(percent_list):\n",
    "        \"\"\"\n",
    "        Calculate percent of each element in the list.\n",
    "        :param percent_list: list for calculating percents.\n",
    "        :return: dictionary with elements of list as keys and percents as values.\n",
    "        \"\"\"\n",
    "        total_frames = len(percent_list)\n",
    "        percentage = {}\n",
    "        for element in percent_list:\n",
    "            if element in percentage.keys():\n",
    "                percentage[element] += 1\n",
    "            else:\n",
    "                percentage[element] = 1\n",
    "        for element in percentage.keys():\n",
    "            percentage[element] = (percentage[element] / total_frames) * 100\n",
    "\n",
    "        return percentage\n",
    "\n",
    "    def process_emotions(self, frames):\n",
    "        \"\"\"\n",
    "        Evaluate emotionality of video fragment.\n",
    "        :param frames: list of frames for evaluation.\n",
    "        :return: string value - emotionality.\n",
    "        \"\"\"\n",
    "        total_frames = len(frames)\n",
    "        emotion_class = VideoEmotions()\n",
    "        emotion_results = []\n",
    "        fps = int(self.fps)\n",
    "        for i in range(0, total_frames, int(fps)):\n",
    "            sec_frames = self.get_subarray(frames, fps, i)[::self.dist]\n",
    "            emotions, scores = emotion_class.process_frames(sec_frames)\n",
    "            emotions = [emo[0] for emo in emotions]\n",
    "            percentages = VideoSubsystem.calculate_percentage(emotions)\n",
    "            try:\n",
    "                max_emotion = max(percentages, key=percentages.get)\n",
    "                emotion_results.append(max_emotion)\n",
    "            except Exception as ex:\n",
    "                print(ex.args[0])\n",
    "                emotion_results.append('emotion not determined')\n",
    "        frequency = emotion_class.calculate_emotion_change_frequency(emotion_results)\n",
    "        percentages = VideoSubsystem.calculate_percentage(emotion_results)\n",
    "        features = [frequency]\n",
    "        for element in ['Sadness', 'Disgust', 'Fear', 'Neutral', 'Happiness', 'Anger',\n",
    "        'Contempt']:\n",
    "            if element in percentages.keys():\n",
    "                features.append(percentages[element])\n",
    "            else:\n",
    "                features.append(0.0)\n",
    "        res = self.emotion_model.predict([features])[0]\n",
    "        percent_res = 0.0\n",
    "        for element in self.inappropriate_emotions:\n",
    "            if element in percentages.keys():\n",
    "                percent_res += percentages[element]\n",
    "        return res, percent_res * 0.01\n",
    "\n",
    "    def process_facial_attributes(self, frames):\n",
    "        \"\"\"\n",
    "        Extract facial attributes on a video fragment.\n",
    "        :param frames: list of frames for evaluation.\n",
    "        :return: age, gender and ethnicity presented on the video fragment.\n",
    "        \"\"\"\n",
    "        total_frames = len(frames)\n",
    "        model_name='age_gender_ethnicity_lagenda_enet0_ft'\n",
    "        far = FacialAttributeRecognizer(model_name=model_name)\n",
    "        age_results, gender_result, ethnicity_result = [], [], []\n",
    "        fps = int(self.fps)\n",
    "        for i in range(0, total_frames, int(fps)):\n",
    "            sec_frames = self.get_subarray(frames, fps, i)[::self.dist]\n",
    "            \n",
    "            age_labels, gender_labels, ethnicity_labels, _, _, _ = far.process_frames(sec_frames)\n",
    "            gender_percentages = VideoSubsystem.calculate_percentage(gender_labels)\n",
    "            ethnicity_percentages = VideoSubsystem.calculate_percentage(ethnicity_labels)\n",
    "            if len(age_labels) > 0: \n",
    "                avg_age = np.mean(age_labels, axis=0)\n",
    "                age_results.append(avg_age)\n",
    "            if len(gender_percentages) > 0:\n",
    "                max_gender = max(gender_percentages, key=gender_percentages.get)\n",
    "                gender_result.append(max_gender)\n",
    "            if len(ethnicity_percentages) > 0:\n",
    "                max_ethnicity = max(ethnicity_percentages, key=ethnicity_percentages.get)\n",
    "                ethnicity_result.append(max_ethnicity)\n",
    "\n",
    "        gender_percentages = VideoSubsystem.calculate_percentage(gender_result)\n",
    "        ethnicity_percentages = VideoSubsystem.calculate_percentage(ethnicity_result)\n",
    "        if len(age_results) > 0:\n",
    "            avg_age = np.mean(age_results, axis=0)\n",
    "        else:\n",
    "            avg_age = None\n",
    "        if len(gender_percentages) > 0:\n",
    "            max_gender = max(gender_percentages, key=gender_percentages.get)\n",
    "        else:\n",
    "            max_gender = None\n",
    "        if len(ethnicity_percentages) > 0:\n",
    "            max_ethnicity = max(ethnicity_percentages, key=ethnicity_percentages.get)\n",
    "        else:\n",
    "            max_ethnicity = None\n",
    "        return avg_age, max_gender, max_ethnicity\n",
    "\n",
    "    def replace_values_with_condition(self, dictionary):\n",
    "        \"\"\"\n",
    "        Change values for values in rating scale.\n",
    "        :param dictionary: dictionary with unprocessed values.\n",
    "        :return: dictionary with processed values.\n",
    "        \"\"\"\n",
    "        for key, value in dictionary.items():\n",
    "            min_val = VideoSubsystem.acceptable_velocity[key][0]\n",
    "            max_val = VideoSubsystem.acceptable_velocity[key][1]\n",
    "            for i in range(len(value)):\n",
    "                if value[i] < min_val:\n",
    "                    value[i] = '0'\n",
    "                elif value[i] > max_val:\n",
    "                    value[i] = '2'\n",
    "                else:\n",
    "                    value[i] = '1'\n",
    "            dictionary[key] = value\n",
    "        return dictionary\n",
    "\n",
    "    def process_gesticulation(self, frames, duration=10):\n",
    "        \"\"\"\n",
    "        Estimate velocity of the speaker.\n",
    "        :param frames: list of frames for estimation.\n",
    "        :param duration: number of seconds for estimation.\n",
    "        :return: estimated velocity.\n",
    "        \"\"\"\n",
    "        gesture = Gestures()\n",
    "        total_frames = len(frames)\n",
    "        fps = int(self.fps)\n",
    "        for i in range(0, total_frames, fps):\n",
    "            sec_frames = self.get_subarray(frames, fps, i)[::self.dist]\n",
    "            gesture.process_velocity(sec_frames)\n",
    "        res = gesture.get_result()\n",
    "        res = self.replace_values_with_condition(res)\n",
    "        result = []\n",
    "        key = list(res.keys())[0]\n",
    "        cycle = len(res[key])\n",
    "        for ind in range(cycle):\n",
    "            percent = []\n",
    "            for key in res.keys():\n",
    "                percent.append(res[key][ind])\n",
    "            percentage = VideoSubsystem.calculate_percentage(percent)\n",
    "            if '2' in percentage.keys():\n",
    "                result.append(2)\n",
    "            elif '0' in percentage.keys() and percentage['0'] > 70:\n",
    "                result.append(0)\n",
    "            else:\n",
    "                result.append(1)\n",
    "        all_percent = VideoSubsystem.calculate_percentage(result)\n",
    "        if 2 in all_percent.keys():\n",
    "            return 2\n",
    "        elif 0 in all_percent.keys() and all_percent[0] > 70:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    def process_gaze(self, frames):\n",
    "        \"\"\"\n",
    "        Calculate percent of incorrect gaze.\n",
    "        :param frames: list of frames for processing.\n",
    "        :return: float value - percent of incorrect frames.\n",
    "        \"\"\"\n",
    "        model = GazeDirection()\n",
    "        percent = model.gaze_detection(frames)\n",
    "        percentages = VideoSubsystem.calculate_percentage(percent)\n",
    "        # max_key = max(percentages, key=percentages.get)\n",
    "        return (100 - percentages.get(\"center\", 0)) * 0.01\n",
    "\n",
    "    def process_angle(self, frames):\n",
    "        \"\"\"\n",
    "        Calculate incorrect angles.\n",
    "        :param frames: list of frames for processing.\n",
    "        :return: float value - percent of incorrect frames.\n",
    "        \"\"\"\n",
    "        perspective = Perspective()\n",
    "        brightness = perspective.count_brightness(frames[::self.dist])\n",
    "        percent, length, inc_ind = perspective.count_angle(frames)\n",
    "        return percent, length, brightness, inc_ind\n",
    "\n",
    "    def process_clothes(self, frames):\n",
    "        \"\"\"\n",
    "        Defines if clothes is appropriate.\n",
    "        :param frames: list of frames for processing.\n",
    "        :return: bool value if clothes is appropriate.\n",
    "        \"\"\"\n",
    "        clothes = Clothes()\n",
    "        return clothes.assess_appearance(frames)\n",
    "\n",
    "\n",
    "    def process_video(self, duration=10):\n",
    "        \"\"\"\n",
    "        Read for duration seconds and process frames.\n",
    "        :param output_path: new path of processed video.\n",
    "        :param duration: number of seconds to process in one cycle.\n",
    "        :return: dictionary with results.\n",
    "        \"\"\"\n",
    "        cap = cv2.VideoCapture(self.video_path)\n",
    "        try:\n",
    "            self.fps = math.ceil(cap.get(cv2.CAP_PROP_FPS))\n",
    "            segment_duration = duration\n",
    "            segment_frame_count = math.ceil(cap.get(cv2.CAP_PROP_FPS) * segment_duration)\n",
    "            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            for i in tqdm(range(0, frame_count, segment_frame_count)):\n",
    "                frames = []\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "                for j in range(segment_frame_count):\n",
    "                    ret, frame = cap.read()\n",
    "                    if not ret:\n",
    "                        break\n",
    "                    frames.append(frame)\n",
    "                if self.emotions:\n",
    "                    res, percent_res = self.process_emotions(frames)\n",
    "                    self.emotion_list.append(res)\n",
    "                    self.emotion_inappropriate_percentage.append(percent_res)\n",
    "                if self.facial_attrs:\n",
    "                    age, gender, ethnicity = self.process_facial_attributes(frames)\n",
    "                    if age is not None:\n",
    "                        self.age_list.append(age)\n",
    "                    if gender is not None:\n",
    "                        self.gender_list.append(gender)\n",
    "                    if ethnicity is not None:\n",
    "                        self.ethnicity_list.append(ethnicity)\n",
    "                if self.gesticulation:\n",
    "                    res = self.process_gesticulation(frames)\n",
    "                    self.gesture_list.append(res)\n",
    "                if self.angle:\n",
    "                    res, length, brightness, inc_ind = self.process_angle(frames)\n",
    "                    self.angle_list.append(res)\n",
    "                    self.angle_len.append(length)\n",
    "                    self.lightning.append(brightness)\n",
    "                    self.inc_ind.append(inc_ind)\n",
    "                if self.gaze:\n",
    "                    res = self.process_gaze(frames)\n",
    "                    self.gaze_list.append(res)\n",
    "                if self.clothes and self.clothes_estimation is None:\n",
    "                    self.clothes_estimation = self.process_clothes(frames)\n",
    "        finally:\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QXceJ2hYAh7R",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Speech Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "3ZdTcjNXAj3e"
   },
   "outputs": [],
   "source": [
    "class AutomaticSpeechRecognition:\n",
    "    \"\"\"\n",
    "    Class for transcribing audio into text.\n",
    "    Creates speech text, words time intervals, unidentified noise time intervals.\n",
    "    \"\"\"\n",
    "    def __init__(self, path, lang):\n",
    "        \"\"\"\n",
    "        Initialization of speech processing class\n",
    "        @param path: path to audio file\n",
    "        \"\"\"\n",
    "        clip = mp_editor.AudioFileClip(path)\n",
    "        self.path = path\n",
    "        self.duration = clip.duration\n",
    "        self.transcription = None\n",
    "        self.lang = lang\n",
    "\n",
    "    def get_speech_recognition(self):\n",
    "        \"\"\"\n",
    "        Translates audio to text, creates words lists with timestamps (with and without background noise)\n",
    "        \"\"\"\n",
    "        model = whisper_timestamped.load_model(\"large\")\n",
    "        # whisper timestamped allows to receive timestamps for each word and sentence, as well as\n",
    "        # noise timestamps\n",
    "        audio = whisper_timestamped.load_audio(self.path)\n",
    "        self.transcription = whisper_timestamped.transcribe(\n",
    "            model,\n",
    "            audio,\n",
    "            language=self.lang,\n",
    "            detect_disfluencies=True,\n",
    "            remove_punctuation_from_words=False)\n",
    "        correct_transcription = self.check_transcription()\n",
    "        # creation of transcription without punctuation marks\n",
    "        transcription = self.transcription[\"text\"].lower()\n",
    "        transcription = transcription.translate(str.maketrans('', '', string.punctuation))\n",
    "        transcription = \"\".join([ch for ch in transcription if ch not in string.digits])\n",
    "        cleaned_transcription = \" \".join(transcription.split())\n",
    "        word_arrays = self.get_words()\n",
    "        return cleaned_transcription, word_arrays, correct_transcription\n",
    "\n",
    "    def check_transcription(self):\n",
    "        \"\"\"\n",
    "        Checks if transcription is correct (if there are word doubles at the end of transcription)\n",
    "        @return: True if transcription is correct, False otherwise\n",
    "        \"\"\"\n",
    "        words = self.transcription[\"text\"].split()\n",
    "        segments = self.transcription[\"segments\"]\n",
    "        end_idx = len(segments)\n",
    "        # find first segment out of time range\n",
    "        for i in range(len(segments)):\n",
    "            if segments[i][\"end\"] > self.duration:\n",
    "                end_idx = i\n",
    "                break\n",
    "        # checks if there is no segments out of time range\n",
    "        if end_idx == len(segments):\n",
    "            return True\n",
    "        else:\n",
    "            # count words out of time range\n",
    "            extra_words = 0\n",
    "            for i in range(end_idx, len(segments)):\n",
    "                extra_words += len(segments[i][\"text\"].split())\n",
    "            # transcription correction\n",
    "            self.transcription[\"text\"] = \" \".join((self.transcription[\"text\"].split())[:len(words) - extra_words])\n",
    "            self.transcription[\"segments\"] = self.transcription[\"segments\"][:end_idx]\n",
    "            return False\n",
    "\n",
    "    def get_words(self):\n",
    "        \"\"\"\n",
    "        Creates lists with all words (with background noise), words without noise and only noise\n",
    "        @return: three lists with dicts of words and their timestamps\n",
    "        \"\"\"\n",
    "        all_words, all_words_without_noise, noise = [], [], []\n",
    "        for sentence in self.transcription[\"segments\"]:\n",
    "            for word in sentence[\"words\"]:\n",
    "                all_words.append(word)\n",
    "                if word[\"text\"] != \"[*]\":\n",
    "                    all_words_without_noise.append(word)\n",
    "                else:\n",
    "                    noise.append((word[\"start\"], word[\"end\"]))\n",
    "        return all_words_without_noise, noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Y9BuXWo6ApcF"
   },
   "outputs": [],
   "source": [
    "class BackgroundNoise:\n",
    "    \"\"\"\n",
    "    Class for background noise detecting.\n",
    "    \"\"\"\n",
    "    # boundary values for size of time window and maximal acceptable noise percentage\n",
    "    params = {\n",
    "        \"noise_time_window\": 30,  # size of time window to view noise percentage\n",
    "        \"noise_percentage\": 0.45,  # maximal noise percentage\n",
    "    }\n",
    "\n",
    "    def __init__(self, noise):\n",
    "        \"\"\"\n",
    "        Initialization of background noise analysis class\n",
    "        @param noise: timestamps with background noise, list of two-element lists\n",
    "        \"\"\"\n",
    "        self.noise = noise\n",
    "\n",
    "    def get_high_noise_timestamps(self):\n",
    "        \"\"\"\n",
    "        Searches most noisy periods with the help of floating window\n",
    "        @return: Most noisy periods, list of two-element lists\n",
    "        \"\"\"\n",
    "        high_noise_timestamps = []\n",
    "        if len(self.noise) == 0:\n",
    "            return high_noise_timestamps\n",
    "        start_idx, end_idx = 0, 1\n",
    "        noise_sum = self.noise[0][1] - self.noise[0][0]\n",
    "        while end_idx < len(self.noise):\n",
    "            # searches for minimal time window larger than boundary value\n",
    "            if self.noise[end_idx][1] - self.noise[start_idx][0] < \\\n",
    "                    self.params[\"noise_time_window\"]:\n",
    "                noise_sum += self.noise[end_idx][1] - self.noise[end_idx][0]\n",
    "                end_idx += 1\n",
    "                continue\n",
    "            # check if the percentage of noise is larger than parameter\n",
    "            if noise_sum / (self.noise[end_idx][1] - self.noise[start_idx][0]) > \\\n",
    "                    self.params[\"noise_percentage\"]:\n",
    "                # if period intersects with previous one - they are united\n",
    "                if len(high_noise_timestamps) > 0 and high_noise_timestamps[-1][1] > \\\n",
    "                        self.noise[start_idx][0]:\n",
    "                    high_noise_timestamps[-1][1] = self.noise[end_idx][1]\n",
    "                # otherwise, new time period is appended\n",
    "                else:\n",
    "                    high_noise_timestamps.append(\n",
    "                        [self.noise[start_idx][0], self.noise[end_idx][1]])\n",
    "            noise_sum -= (self.noise[start_idx][1] - self.noise[start_idx][0])\n",
    "            start_idx += 1\n",
    "        return high_noise_timestamps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "KcfYui14Aq2M"
   },
   "outputs": [],
   "source": [
    "class AudioEmotions:\n",
    "    \"\"\"\n",
    "    Class for emotions detecting.\n",
    "    Counts percentage of preferred emotions and percentage of neutral emotion.\n",
    "    \"\"\"\n",
    "    def __init__(self, path, analyzed_segment_len, negative_emotions):\n",
    "        \"\"\"\n",
    "        Initialization of emotion classification class\n",
    "        @param path: path to audio file\n",
    "        @param analyzed_segment_len: length of file segment to analyze separately\n",
    "        @param negative_emotions: list of preferred emotions\n",
    "        \"\"\"\n",
    "        self.path = path\n",
    "        self.analyzed_segment_len = analyzed_segment_len\n",
    "        self.negative_emotions = negative_emotions\n",
    "        # paths for N-second sub clips\n",
    "        path = os.path.abspath(os.path.dirname(__file__))\n",
    "        self.subclip_path = os.path.abspath(os.path.join(path, \"file_processing/processing.wav\"))\n",
    "        self.subclip_modified_path = os.path.abspath(os.path.join(path, \"file_processing/processing2.wav\"))\n",
    "        # order of emotions in model\n",
    "        self.order = [\"happiness\", \"anger\", \"disgust\", \"neutral\", \"sadness\", \"enthusiasm\"]\n",
    "\n",
    "    def emotions_analysis(self):\n",
    "        \"\"\"\n",
    "        Analyzes speech per N seconds (see init params) and provides emotions probabilities\n",
    "        @return: lists with emotions probabilities\n",
    "        \"\"\"\n",
    "        model = VoiceRecognizer(model=HuggingFaceModel.Voice.Wav2Vec2)\n",
    "        clip = AudioFileClip(self.path)\n",
    "        duration = clip.duration\n",
    "        # number of file fragments to analyze\n",
    "        number_of_segments = math.ceil(duration / self.analyzed_segment_len)\n",
    "        negative_emotions_percentage = np.zeros(number_of_segments)\n",
    "        neutral_emotion_percentage = np.zeros(number_of_segments)\n",
    "        time = self.analyzed_segment_len\n",
    "        for i in tqdm(range(number_of_segments)):\n",
    "            # path to analyzed file fragment\n",
    "            subclip = clip.subclip(i * time, min(i * time + time, duration))\n",
    "            subclip.write_audiofile(self.subclip_path, logger=None)\n",
    "\n",
    "            # sub clip preprocessing to convert stereo to mono\n",
    "            self.audio_channels_processing()\n",
    "            emotions_percentages = model.recognize(self.subclip_modified_path, return_single_label=False)\n",
    "            # counting of preferred emotions percentage\n",
    "            for idx, emotion in enumerate(self.order):\n",
    "                if self.negative_emotions[idx]:\n",
    "                    negative_emotions_percentage[i] += emotions_percentages[emotion]\n",
    "                neutral_emotion_percentage[i] = emotions_percentages[\"neutral\"]\n",
    "\n",
    "        # deleting of intermediate files\n",
    "        file_paths = [self.subclip_path, self.subclip_modified_path]\n",
    "        for file_path in file_paths:\n",
    "            if os.path.isfile(file_path):\n",
    "                os.remove(file_path)\n",
    "        return negative_emotions_percentage, neutral_emotion_percentage\n",
    "\n",
    "    def audio_channels_processing(self):\n",
    "        \"\"\"\n",
    "        Rewriting file to one channel if necessary\n",
    "        \"\"\"\n",
    "        audio_file = wave.open(self.subclip_path)\n",
    "        channels = audio_file.getnchannels()\n",
    "        sound = AudioSegment.from_wav(self.subclip_path)\n",
    "        if channels > 1:\n",
    "            sound = sound.set_channels(1)\n",
    "        # rewriting one channel file\n",
    "        sound.export(self.subclip_modified_path, format=\"wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "xhzIdkd7AsFw"
   },
   "outputs": [],
   "source": [
    "class FillerWordsAndPhrases:\n",
    "    \"\"\"\n",
    "    Class for filler words and phrases detecting.\n",
    "    Detects words and phrases from lists and as most common in speech.\n",
    "    \"\"\"\n",
    "    # maximal acceptable percentages and lists of filler words\n",
    "    params_ru = {\n",
    "        # multiplier for most common word or phrase occurrence to be compared with others\n",
    "        \"word_count_multiplier\": 0.1,\n",
    "        # minimal percentage for word or phrase to be considered common\n",
    "        \"occurrence_percentage\": 0.0001,\n",
    "        \"parasites\": [\"просто\", \"вот\", \"ну\", \"короче\", \"типа\", \"пожалуй\", \"кстати\", \"вообще\", \"буквально\", \"скажем\",\n",
    "                      \"блин\", \"допустим\", \"черт\", \"вроде\", \"круто\", \"прикинь\", \"прикиньте\", \"реально\", \"отпад\",\n",
    "                      \"отпадно\", \"клево\", \"капец\", \"норм\", \"слушай\", \"конечно\", \"наверное\", \"вероятно\", \"кажется\"],\n",
    "        \"parasite_phrases\": [\"так сказать\", \"как бы\", \"в натуре\", \"в общем\", \"в общемто\", \"в целом\", \"в принципе\",\n",
    "                             \"как говорится\", \"как сказать\", \"на фиг\", \"то есть\", \"это самое\", \"как его\", \"типа того\"]\n",
    "    }\n",
    "    params_en = {\n",
    "        # multiplier for most common word or phrase occurrence to be compared with others\n",
    "        \"word_count_multiplier\": 0.1,\n",
    "        # minimal percentage for word or phrase to be considered common\n",
    "        \"occurrence_percentage\": 0.0001,\n",
    "        \"parasites\": [\"like\", \"um\", \"uh\", \"well\", \"so\", \"just\", \"actually\", \"literally\", \"basically\", \"really\",\n",
    "                      \"seriously\", \"okay\", \"right\", \"honestly\", \"sure\", \"maybe\", \"perhaps\", \"kinda\", \"sorta\", \"totally\"],\n",
    "        \"parasite_phrases\": [\"you know\", \"i mean\", \"you see\", \"i guess\", \"i suppose\", \"the thing is\",\n",
    "                             \"let's say\", \"it's like\", \"at the end of the day\", \"and stuff\", \"or something\",\n",
    "                             \"you know what i mean\", \"to be honest\", \"in a way\", \"if you will\"]\n",
    "    }\n",
    "\n",
    "    def __init__(self, cleaned_transcription, lang):\n",
    "        \"\"\"\n",
    "        Initialization of filler words detection class\n",
    "        @param cleaned_transcription: text transcription without punctuation marks\n",
    "        \"\"\"\n",
    "        self.cleaned_transcription = cleaned_transcription\n",
    "        if lang == \"ru\":\n",
    "            self.params = self.params_ru\n",
    "        else:\n",
    "            self.params = self.params_en\n",
    "\n",
    "    def count_occurrences(self, min_len=5):\n",
    "        \"\"\"\n",
    "        Counts two-words phrases occurrences\n",
    "        @param min_len: minimal length in letters for phrase to be considered\n",
    "        @return: list of two-element lists, each with phrase and its occurrence\n",
    "        \"\"\"\n",
    "        pairs = dict()\n",
    "        words = self.cleaned_transcription.split()\n",
    "        for i in range(len(words) - 1):\n",
    "            # create two-word phrases\n",
    "            phrase = words[i] + ' ' + words[i + 1]\n",
    "            if len(phrase) > min_len:\n",
    "                # save phrases with acceptable length\n",
    "                if phrase not in pairs:\n",
    "                    pairs[phrase] = 0\n",
    "                pairs[phrase] += 1\n",
    "        phrases_from_list = {}\n",
    "\n",
    "        # rewrite phrases from list into separate dictionary\n",
    "        for phrase in self.params[\"parasite_phrases\"]:\n",
    "            if phrase in pairs:\n",
    "                phrases_from_list[phrase] = pairs[phrase]\n",
    "        phrase_dic = list(pairs.items())\n",
    "        phrases = sorted(phrase_dic, key=lambda x: -x[1])\n",
    "        return phrases, phrases_from_list\n",
    "\n",
    "    def find_worst_phrases(self, phrases):\n",
    "        \"\"\"\n",
    "        Takes most common phrases from all\n",
    "        @param phrases: all two-word phrases\n",
    "        @return: dictionary with key - phrases and value - their occurrences\n",
    "        \"\"\"\n",
    "        num_words = len(self.cleaned_transcription)\n",
    "        max_repeats = phrases[0][1]\n",
    "        # if all collocations appear one time - there are no most common phrases\n",
    "        if max_repeats == 1 or max_repeats / num_words < self.params[\"occurrence_percentage\"]:\n",
    "            return dict()\n",
    "        # maximal deviation from most common word or phrase occurrence\n",
    "        diff = round(max_repeats * self.params[\"word_count_multiplier\"])\n",
    "        worst_word_pairs = dict()\n",
    "        # find phrases with small deviation from most common one\n",
    "        for word_pair, cnt in phrases:\n",
    "            if cnt >= max_repeats - diff and cnt / num_words >= self.params[\"occurrence_percentage\"]:\n",
    "                worst_word_pairs[word_pair] = cnt\n",
    "        return worst_word_pairs\n",
    "\n",
    "    def get_one_words(self):\n",
    "        \"\"\"\n",
    "        Counts all filler words from params parasites\n",
    "        @return: frequency dictionary with key - words and value - their occurrences\n",
    "        \"\"\"\n",
    "        text_tokens = word_tokenize(self.cleaned_transcription)\n",
    "        text_tokens = [token.strip() for token in text_tokens if token in set(self.params[\"parasites\"])]\n",
    "        text = nltk.Text(text_tokens)\n",
    "        fdist = FreqDist(text)\n",
    "        return fdist\n",
    "\n",
    "    def find_worst_words(self, fdist):\n",
    "        \"\"\"\n",
    "        Takes most common filler words from all\n",
    "        @param fdist: frequency dictionary with key - words and value - their occurrences\n",
    "        @return: dictionary with key - words and value - their occurrences\n",
    "        \"\"\"\n",
    "        num_words = len(self.cleaned_transcription)\n",
    "        if len(fdist) == 0:\n",
    "            return dict()\n",
    "        # most common word appearance\n",
    "        max_repeats = fdist.most_common(1)[0][1]\n",
    "        if max_repeats == 1 or max_repeats / num_words < self.params[\"occurrence_percentage\"]:\n",
    "            return dict()\n",
    "        # maximal deviation from most common word or phrase occurrence\n",
    "        diff = round(max_repeats * self.params[\"word_count_multiplier\"])\n",
    "        idx = 1\n",
    "        # add words with high occurrence percentage\n",
    "        while idx <= len(fdist) and fdist.most_common(idx)[-1][1] >= max_repeats - diff and \\\n",
    "                fdist.most_common(idx)[-1][1] / num_words >= self.params[\"occurrence_percentage\"]:\n",
    "            idx += 1\n",
    "        worst_words = dict(fdist.most_common(idx - 1))\n",
    "        return worst_words\n",
    "\n",
    "    def get_filler_words_final(self):\n",
    "        \"\"\"\n",
    "        Concatenates all words and phrases into two dictionaries - all and most common filler words\n",
    "        @return: two dictionaries with words / phrases and their occurrences\n",
    "        \"\"\"\n",
    "        # find all and most common / listed phrases\n",
    "        phrases, phrases_from_list = self.count_occurrences()\n",
    "        worst_phrases = self.find_worst_phrases(phrases)\n",
    "\n",
    "        # find all and most common / listed words\n",
    "        fdist = self.get_one_words()\n",
    "        worst_words = self.find_worst_words(fdist)\n",
    "\n",
    "        # dicts with all and most common / list words and phrases\n",
    "        total_dict = dict(worst_phrases) | dict(fdist) | phrases_from_list\n",
    "        worst_dict = dict(worst_phrases) | dict(worst_words)\n",
    "        return total_dict, worst_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "9kgD4plzAtYJ"
   },
   "outputs": [],
   "source": [
    "class Intelligibility:\n",
    "    \"\"\"\n",
    "    Class for intelligibility detecting.\n",
    "    Uses info from background noise analysis and high speech rate timestamps.\n",
    "    \"\"\"\n",
    "    def __init__(self, path, all_words_without_noise, noise, analyzed_segment_len):\n",
    "        \"\"\"\n",
    "        Initialization of background noise analysis class\n",
    "        @param path: path to audio file\n",
    "        @param all_words_without_noise: list of all words and their timestamps\n",
    "        @param noise: timestamps with background noise, list of two-element lists\n",
    "        @param analyzed_segment_len: length of file segment to analyze separately\n",
    "        \"\"\"\n",
    "        self.noise = noise\n",
    "        self.path = path\n",
    "        self.all_words_without_noise = all_words_without_noise\n",
    "        self.analyzed_segment_len = analyzed_segment_len\n",
    "\n",
    "    def stoi_index(self):\n",
    "        \"\"\"\n",
    "        Counting short time objective intelligibility index per file fragment\n",
    "        @return: list with STOI indexes for each fragment\n",
    "        \"\"\"\n",
    "        # paths for file fragments\n",
    "        subclip_path = \"processing.wav\"\n",
    "        subclip_modified_path = \"processing2.wav\"\n",
    "\n",
    "        clip = AudioFileClip(self.path)\n",
    "        duration = clip.duration\n",
    "        # number of file segments to analyze\n",
    "        number_of_segments = math.ceil(duration / self.analyzed_segment_len)\n",
    "        indexes = np.zeros(number_of_segments)\n",
    "        for i in range(number_of_segments):\n",
    "            # file fragment (checks for len not out of file length)\n",
    "            subclip = clip.subclip(i * self.analyzed_segment_len,\n",
    "                                   min((i + 1) * self.analyzed_segment_len, clip.duration))\n",
    "            # it is ineffective to analyze too short fragments\n",
    "            if subclip.duration < 3:\n",
    "                indexes[i] = 0.5\n",
    "                continue\n",
    "            subclip.write_audiofile(subclip_path, logger=None)\n",
    "            data, rate = librosa.load(subclip_path)\n",
    "            # cleaning degraded speech signal\n",
    "            reduced_noise = nr.reduce_noise(y=data, sr=rate, thresh_n_mult_nonstationary=2, stationary=False)\n",
    "            wavf.write(subclip_modified_path, rate, reduced_noise)\n",
    "            # loading signal info\n",
    "            clean, fs = librosa.load(subclip_modified_path)\n",
    "            base, fs = librosa.load(subclip_path)\n",
    "            # counting and saving STOI indexes\n",
    "            index = stoi(clean, base, fs, extended=False)\n",
    "            indexes[i] = round(index, 3)\n",
    "\n",
    "        # deleting intermediate files\n",
    "        file_paths = [subclip_path, subclip_modified_path]\n",
    "        for file_path in file_paths:\n",
    "            if os.path.isfile(file_path):\n",
    "                os.remove(file_path)\n",
    "        return indexes\n",
    "\n",
    "    def indirect_features(self):\n",
    "        \"\"\"\n",
    "        Analyses intelligibility of speech\n",
    "        @return: intervals with high speech and high levels of background noise\n",
    "        \"\"\"\n",
    "        # timestamps with fast speech rate\n",
    "        speech_rate = SpeechRate(self.all_words_without_noise)\n",
    "        _, fast_intervals = speech_rate.find_incorrect_speech_rate_intervals()\n",
    "        # timestapms with high background noise\n",
    "        noisy_intervals = BackgroundNoise(self.noise).get_high_noise_timestamps()\n",
    "\n",
    "        return fast_intervals, noisy_intervals\n",
    "\n",
    "    def get_intelligibility_features(self):\n",
    "        \"\"\"\n",
    "        Final method for aggregating file info\n",
    "        @return: lists with STOI indexes, intervals with high speech and high levels of background noise\n",
    "        \"\"\"\n",
    "        indexes = self.stoi_index()\n",
    "        fast_intervals, noisy_intervals = self.indirect_features()\n",
    "        return indexes, fast_intervals, noisy_intervals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "xvN4lmS6Axqg"
   },
   "outputs": [],
   "source": [
    "class SpeechRate:\n",
    "    \"\"\"\n",
    "    Analyses speech rate, searches for fast and slow speech rate intervals.\n",
    "    \"\"\"\n",
    "    # border parameters for analysis\n",
    "    params = {\n",
    "        # size of time window to view pauses\n",
    "        \"pause_time_window\": 30,\n",
    "        # minimal noise percentage\n",
    "        \"pause_percentage\": 0.35,\n",
    "        # size of time window to speech rate\n",
    "        \"speech_rate_time_window\": 60,\n",
    "        # minimal number of words in time window for normal speech rate\n",
    "        \"speech_rate_min_word_count\": 60,\n",
    "        # maximal number of words in time window for normal speech rate\n",
    "        \"speech_rate_max_word_count\": 140,\n",
    "        # allowed pauses between words\n",
    "        \"rules\": {\n",
    "            \"word\": 0.5,\n",
    "            \"punct_mark\": 0.75,\n",
    "            \".\": 1,\n",
    "            \"?\": 5,\n",
    "            \"!\": 3\n",
    "        },\n",
    "    }\n",
    "\n",
    "    def __init__(self, all_words_without_noise):\n",
    "        \"\"\"\n",
    "        Initialization of speech rate analysis class\n",
    "        @param all_words_without_noise: list of dicts with words and their start and end timestamps\n",
    "        \"\"\"\n",
    "        self.all_words_without_noise = all_words_without_noise\n",
    "\n",
    "    def find_pauses(self):\n",
    "        \"\"\"\n",
    "        Finds all pauses longer than allowed\n",
    "        @return: list of two-element lists with pauses timestamps\n",
    "        \"\"\"\n",
    "        rules = self.params[\"rules\"]\n",
    "        pauses = []\n",
    "        start_idx = 0\n",
    "        end_idx = 1\n",
    "        while end_idx < len(self.all_words_without_noise) - 1:\n",
    "            silence_start = self.all_words_without_noise[start_idx][\"end\"]\n",
    "            silence_end = self.all_words_without_noise[end_idx][\"start\"]\n",
    "            # detecting pause type\n",
    "            if self.all_words_without_noise[start_idx][\"text\"][-1].isalpha():\n",
    "                pause_type = rules[\"word\"]\n",
    "            elif self.all_words_without_noise[start_idx][\"text\"][-1] in rules:\n",
    "                pause_type = rules[self.all_words_without_noise[start_idx][\"text\"][-1]]\n",
    "            else:\n",
    "                pause_type = rules[\"punct_mark\"]\n",
    "            # checking with border value (depends on pause type)\n",
    "            if silence_end - silence_start > pause_type:\n",
    "                pauses.append([silence_start, silence_end])\n",
    "            start_idx = end_idx\n",
    "            end_idx += 1\n",
    "        return pauses\n",
    "\n",
    "    def find_pause_intervals(self, pauses):\n",
    "        \"\"\"\n",
    "        Searches periods with high pauses percentage with the help of floating window\n",
    "        @param pauses: pauses intervals, list of two-element lists\n",
    "        @return: list of two-element lists with pause intervals timestamps\n",
    "        \"\"\"\n",
    "        intervals = []\n",
    "        if len(pauses) == 0:\n",
    "            return intervals\n",
    "        start_idx, end_idx = 0, 0\n",
    "        # current pause length\n",
    "        summary = pauses[0][1] - pauses[0][0]\n",
    "        while end_idx < len(pauses):\n",
    "            while end_idx < len(pauses) - 1 and pauses[end_idx][1] - pauses[start_idx][0] < \\\n",
    "                    self.params[\"pause_time_window\"]:\n",
    "                end_idx += 1\n",
    "                summary += pauses[end_idx][1] - pauses[end_idx][0]\n",
    "            # break if file end is reached\n",
    "            if pauses[end_idx][1] - pauses[start_idx][0] < self.params[\"pause_time_window\"]:\n",
    "                break\n",
    "            # check if the percentage of pauses is larger than parameter\n",
    "            if summary / (pauses[end_idx][1] - pauses[start_idx][0]) > self.params[\"pause_percentage\"]:\n",
    "                # if period intersects with previous one - they are united\n",
    "                if len(intervals) > 0 and intervals[-1][-1] > pauses[start_idx][0]:\n",
    "                    intervals[-1][-1] = pauses[end_idx][1]\n",
    "                else:\n",
    "                    intervals.append([pauses[start_idx][0], pauses[end_idx][1]])\n",
    "            # delete first pause, move interval start to next word\n",
    "            summary -= pauses[start_idx][1] - pauses[start_idx][0]\n",
    "            start_idx += 1\n",
    "        return intervals\n",
    "\n",
    "    def find_incorrect_speech_rate_intervals(self):\n",
    "        \"\"\"\n",
    "        Searches intervals with too fast or slow speech rate\n",
    "        @return: two lists with two-element list each - periods with too fast or slow speech rate\n",
    "        \"\"\"\n",
    "        fast_intervals = []\n",
    "        slow_intervals = []\n",
    "        word_count = 1\n",
    "        start = self.all_words_without_noise[0][\"start\"]\n",
    "        end = self.all_words_without_noise[0][\"end\"]\n",
    "        start_idx = 0\n",
    "        end_idx = 1\n",
    "        while end_idx < len(self.all_words_without_noise):\n",
    "            # add word if time window is smaller than border value\n",
    "            if end - start < self.params[\"speech_rate_time_window\"]:\n",
    "                end = self.all_words_without_noise[end_idx][\"end\"]\n",
    "                end_idx += 1\n",
    "                word_count += 1\n",
    "            else:\n",
    "                # if word count is too small or too large - append time interval to corresponding list\n",
    "                if word_count < self.params[\"speech_rate_min_word_count\"]:\n",
    "                    # unite intervals if necessary\n",
    "                    if len(slow_intervals) > 0 and slow_intervals[-1][1] >= start:\n",
    "                        slow_intervals[-1][1] = end\n",
    "                    else:\n",
    "                        slow_intervals.append([start, end])\n",
    "                elif word_count > self.params[\"speech_rate_max_word_count\"]:\n",
    "                    # unite intervals if necessary\n",
    "                    if len(fast_intervals) > 0 and fast_intervals[-1][1] >= start:\n",
    "                        fast_intervals[-1][1] = end\n",
    "                    else:\n",
    "                        fast_intervals.append([start, end])\n",
    "                # remove first word from interval\n",
    "                start_idx += 1\n",
    "                start = self.all_words_without_noise[start_idx][\"start\"]\n",
    "                word_count -= 1\n",
    "        return slow_intervals, fast_intervals\n",
    "\n",
    "    def get_intervals(self):\n",
    "        \"\"\"\n",
    "        get slow intervals in two formats - high pauses percentage and low speech rate\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        speech_rate_results, _ = self.find_incorrect_speech_rate_intervals()\n",
    "        pauses = self.find_pauses()\n",
    "        pause_intervals = self.find_pause_intervals(pauses)\n",
    "        return speech_rate_results, pause_intervals\n",
    "\n",
    "    def unite_slow_speech_rate_intervals(self):\n",
    "        \"\"\"\n",
    "        Unites two lists of intervals: with pauses and with slow speech rate\n",
    "        @return: list of two-element lists with slow speech rate intervals timestamps\n",
    "        \"\"\"\n",
    "        speech_rate_results, _ = self.find_incorrect_speech_rate_intervals()\n",
    "        pauses = self.find_pauses()\n",
    "        pause_intervals = self.find_pause_intervals(pauses)\n",
    "        final_intervals = []\n",
    "        speech_rate_idx, pause_idx = 0, 0\n",
    "        while speech_rate_idx < len(speech_rate_results) and pause_idx < len(pause_intervals):\n",
    "            sr_start, sr_end = speech_rate_results[speech_rate_idx]\n",
    "            pause_start, pause_end = pause_intervals[pause_idx][0], pause_intervals[pause_idx][1]\n",
    "            if sr_start <= pause_start:\n",
    "                if sr_end <= pause_start:\n",
    "                    speech_rate_idx += 1\n",
    "                elif pause_start < sr_end <= pause_end:\n",
    "                    final_intervals.append([pause_start, sr_end])\n",
    "                    speech_rate_idx += 1\n",
    "                else:\n",
    "                    final_intervals.append([pause_start, pause_end])\n",
    "                    pause_idx += 1\n",
    "            elif pause_start <= sr_start <= pause_end:\n",
    "                if sr_end <= pause_end:\n",
    "                    final_intervals.append([sr_start, sr_end])\n",
    "                    speech_rate_idx += 1\n",
    "                else:\n",
    "                    final_intervals.append([sr_start, pause_end])\n",
    "                    pause_idx += 1\n",
    "            else:\n",
    "                pause_idx += 1\n",
    "        return final_intervals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "9iQuuW8RAu60"
   },
   "outputs": [],
   "source": [
    "class SpeechProcessingSubsystem:\n",
    "    def __init__(self, path, negative_emotions_bool, analyzed_segment_len, lang):\n",
    "        \"\"\"\n",
    "        Initialization of speech processing class\n",
    "        @param path: path to video file\n",
    "        @param negative_emotions_bool: list of unwanted emotions (set by user)\n",
    "        @param analyzed_segment_len: length of file segment to analyze separately\n",
    "        \"\"\"\n",
    "        # rewrite video to audio file\n",
    "        clip = mp_editor.VideoFileClip(path)\n",
    "        audio_path = path[:path.rfind('.')] + '.wav'\n",
    "        clip.audio.write_audiofile(audio_path, logger=None)\n",
    "        self.path = audio_path\n",
    "        # fields for words and noise timestamps\n",
    "        self.cleaned_transcription = None\n",
    "        self.all_words_without_noise = None\n",
    "        self.noise = None\n",
    "        self.duration = clip.duration\n",
    "        self.analyzed_segment_len = analyzed_segment_len\n",
    "        self.negative_emotions_bool = negative_emotions_bool\n",
    "        self.lang = lang\n",
    "\n",
    "    def speech_recognition(self):\n",
    "        \"\"\"\n",
    "        Performs ASR process\n",
    "        \"\"\"\n",
    "        speech_recogniser = AutomaticSpeechRecognition(self.path, self.lang)\n",
    "        cleaned_transcription, word_arrays, correct_transcription = \\\n",
    "            speech_recogniser.get_speech_recognition()\n",
    "        # fill class params with ASR results\n",
    "        self.cleaned_transcription = cleaned_transcription\n",
    "        self.all_words_without_noise = word_arrays[0]\n",
    "        self.noise = word_arrays[1]\n",
    "\n",
    "    @staticmethod\n",
    "    def unite_intervals(intervals_1, intervals_2):\n",
    "        \"\"\"\n",
    "        Unite two time frames intervals\n",
    "        @param intervals_1: first list of intervals\n",
    "        @param intervals_2: second list of intervals\n",
    "        @return: united list of intervals\n",
    "        \"\"\"\n",
    "        final_intervals = []\n",
    "        # indexes to indexing through lists\n",
    "        first_idx, second_idx = 0, 0\n",
    "        while first_idx < len(intervals_1) and second_idx < len(intervals_2):\n",
    "            interval_1_start, interval_1_end = intervals_1[first_idx]\n",
    "            interval_2_start, interval_2_end = intervals_2[second_idx][0], intervals_2[second_idx][1]\n",
    "            # if first interval's time period is earlier\n",
    "            if interval_1_start <= interval_2_start:\n",
    "                # first interval's time period is inside second's\n",
    "                if interval_1_end <= interval_2_start:\n",
    "                    first_idx += 1\n",
    "                elif interval_2_start < interval_1_end <= interval_2_end:\n",
    "                    final_intervals.append([interval_2_start, interval_1_end])\n",
    "                    first_idx += 1\n",
    "                else:\n",
    "                    final_intervals.append([interval_2_start, interval_2_end])\n",
    "                    second_idx += 1\n",
    "            # if second interval's time period is earlier\n",
    "            elif interval_2_start <= interval_1_start <= interval_2_end:\n",
    "                if interval_1_end <= interval_2_end:\n",
    "                    final_intervals.append([interval_1_start, interval_1_end])\n",
    "                    first_idx += 1\n",
    "                else:\n",
    "                    final_intervals.append([interval_1_start, interval_2_end])\n",
    "                    second_idx += 1\n",
    "            else:\n",
    "                second_idx += 1\n",
    "        return final_intervals\n",
    "\n",
    "    def periods_to_fractions(self, intervals, length):\n",
    "        \"\"\"\n",
    "        Saves percentages of intervals per analyzed file fragment length\n",
    "        @param intervals: time intervals of any kind\n",
    "        @param length: result's list length\n",
    "        @return: list with fractions (percentages) of occurrence\n",
    "        \"\"\"\n",
    "        res = np.zeros(length)\n",
    "        for i in intervals:\n",
    "            fraction = (i[1] - i[0]) / self.analyzed_segment_len\n",
    "            idx = int(i[0] // self.analyzed_segment_len)\n",
    "            res[idx] = round(res[idx] + fraction, 3)\n",
    "        return res\n",
    "\n",
    "    def get_fraction(self, timestamps):\n",
    "        \"\"\"\n",
    "        Counts timestamps proportion of some event\n",
    "        @param timestamps: time periods of some event\n",
    "        @return: timestamps proportion of some event\n",
    "        \"\"\"\n",
    "        duration = 0\n",
    "        for time_period in timestamps:\n",
    "            duration += time_period[1] - time_period[0]\n",
    "        return duration / self.duration\n",
    "\n",
    "    def get_fractions_from_intervals(self, intervals):\n",
    "        \"\"\"\n",
    "        Transform random length intervals to N-second fractions\n",
    "        @param intervals: intervals of some event\n",
    "        @return: list of fraction per file fragment\n",
    "        \"\"\"\n",
    "        length = math.ceil(self.duration / self.analyzed_segment_len)\n",
    "        fixed_intervals = [[i * self.analyzed_segment_len, (i + 1) * self.analyzed_segment_len] for i in range(length)]\n",
    "        united_intervals = self.unite_intervals(intervals, fixed_intervals)\n",
    "        fractions = self.periods_to_fractions(united_intervals, len(fixed_intervals))\n",
    "        return fractions\n",
    "\n",
    "    def get_emotionality(self):\n",
    "        \"\"\"\n",
    "        Analyses emotionality of file\n",
    "        @return: list of lists of emotions probabilities and time period per which emotions are defined\n",
    "        \"\"\"\n",
    "        audio_emotions = AudioEmotions(self.path, self.analyzed_segment_len, self.negative_emotions_bool)\n",
    "        negative_emotions_fractions, neutral_emotion_fractions = audio_emotions.emotions_analysis()\n",
    "        return negative_emotions_fractions, neutral_emotion_fractions\n",
    "\n",
    "    def get_filler_words(self):\n",
    "        \"\"\"\n",
    "        Analyses presence of filler words\n",
    "        @return: dicts with all filler words and phrases and with most common ones\n",
    "        \"\"\"\n",
    "        filler_words = FillerWordsAndPhrases(self.cleaned_transcription, self.lang)\n",
    "        all_filler_words_dict, worst_words = filler_words.get_filler_words_final()\n",
    "        return all_filler_words_dict, worst_words\n",
    "\n",
    "    def get_speech_rate(self):\n",
    "        \"\"\"\n",
    "        Analyses speech rate of speech\n",
    "        @return: intervals with slow speech rate and their percentage of file duration\n",
    "        \"\"\"\n",
    "        speech_rate = SpeechRate(self.all_words_without_noise)\n",
    "        speech_rate_results, pause_intervals = speech_rate.get_intervals()\n",
    "        intervals = self.unite_intervals(speech_rate_results, pause_intervals)\n",
    "        fractions = self.get_fractions_from_intervals(intervals)\n",
    "        return intervals, fractions, self.get_fraction(intervals)\n",
    "\n",
    "    def get_background_noise(self):\n",
    "        \"\"\"\n",
    "        Analyses background noise presence\n",
    "        @return: intervals with high background noise and their percentage of file duration\n",
    "        \"\"\"\n",
    "        # collect high background noise intervals\n",
    "        background_noise = BackgroundNoise(self.noise)\n",
    "        high_noise_intervals = background_noise.get_high_noise_timestamps()\n",
    "        # transform to fractions for each file fragment\n",
    "        high_noise_fractions = self.get_fractions_from_intervals(high_noise_intervals)\n",
    "        high_noise_fractions = np.array(high_noise_fractions)\n",
    "\n",
    "        # collect STOI indexes\n",
    "        intelligibility = Intelligibility(self.path, self.all_words_without_noise, self.noise,\n",
    "                                          self.analyzed_segment_len)\n",
    "        indexes = intelligibility.stoi_index()\n",
    "        # transform to fractions for each file fragment\n",
    "        fractions = (high_noise_fractions + 1 - indexes) / 2\n",
    "        return high_noise_intervals, fractions, self.get_fraction(high_noise_intervals)\n",
    "\n",
    "    def get_intelligibility(self):\n",
    "        \"\"\"\n",
    "        Analyses intelligibility of speech\n",
    "        @return: approximate intelligibility per file fragment and summary intelligibility\n",
    "        \"\"\"\n",
    "        # collect basic intelligibility measures\n",
    "        intelligibility = Intelligibility(self.path, self.all_words_without_noise, self.noise,\n",
    "                                          self.analyzed_segment_len)\n",
    "        indexes, fast_intervals, noisy_intervals = intelligibility.get_intelligibility_features()\n",
    "        # transform to fractions on whole file\n",
    "        fast_fraction = self.get_fraction(fast_intervals)\n",
    "        noisy_fraction = self.get_fraction(noisy_intervals)\n",
    "        index_fraction = np.average(indexes)\n",
    "        # transform to fractions per file fragment\n",
    "        noisy_fractions = np.array(self.get_fractions_from_intervals(noisy_intervals))\n",
    "        fast_fractions = np.array(self.get_fractions_from_intervals(fast_intervals))\n",
    "\n",
    "        # count average\n",
    "        negative_fractions = (2 * noisy_fractions + fast_fractions + 2 * (1 - indexes)) / 5\n",
    "        negative_fraction = (fast_fraction + 2 * noisy_fraction + 2 * (1 - index_fraction)) / 5\n",
    "        return negative_fractions, negative_fraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QPd9BdumEpRe"
   },
   "source": [
    "## Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "biH6QlukMPXz"
   },
   "outputs": [],
   "source": [
    "CONSTANTS = {\n",
    "    \"clean_speech\": (0.3, 0.8),\n",
    "    \"speech_rate\": (0.3, 0.6),\n",
    "    \"background_noise\": (0.3, 0.6),\n",
    "    \"intelligibility\": (0.3, 0.6),\n",
    "    \"clothes\": (0.5, 1),\n",
    "    \"gestures\": (0, 1),\n",
    "    \"angle\": (0.3, 0.6),\n",
    "    \"glances\": (0.6, 1),\n",
    "    \"emotionality\": (0.3, 0.6),\n",
    "    \"neutral_emotionality_official\": (0.2, 0.6),\n",
    "    \"neutral_emotionality_nonofficial\": (0.2, 0.6),\n",
    "}\n",
    "\n",
    "ORDER = [\n",
    "    \"background_noise\",\n",
    "    \"speech_rate\",\n",
    "    \"emotionality\",\n",
    "    \"intelligibility\",\n",
    "    \"gestures\",\n",
    "    \"glances\"\n",
    "]\n",
    "\n",
    "DRAW_VALUES = {\n",
    "    \"speech_rate\": {\n",
    "        0: \"Optimal speech rate\",\n",
    "        1: \"Slightly slow speech rate\",\n",
    "        2: \"Too slow speech rate\"\n",
    "    },\n",
    "    \"background_noise\": {\n",
    "        0: \"No background noise\",\n",
    "        1: \"Slight background noise\",\n",
    "        2: \"Strong background noise\"\n",
    "    },\n",
    "    \"intelligibility\": {\n",
    "        0: \"Speech is completely unintelligible\",\n",
    "        1: \"Speech is somewhat unintelligible\",\n",
    "        2: \"Speech is fully intelligible\",\n",
    "    },\n",
    "    \"gestures\": {\n",
    "        0: 'Inactive gesturing',\n",
    "        1: 'Optimal gesturing',\n",
    "        2: 'Active gesturing',\n",
    "    },\n",
    "    \"glances\": {\n",
    "        0: None,\n",
    "        1: 'You often look away'\n",
    "    },\n",
    "    \"emotionality\": {\n",
    "        0: \"Mostly desired emotions\",\n",
    "        1: \"Not fully desired emotions\",\n",
    "        2: \"Undesired emotions\",\n",
    "    },\n",
    "    \"lightning\": {\n",
    "        0: 'Too dark lighting',\n",
    "        1: 'Optimal lighting',\n",
    "        2: 'Too bright lighting',\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "KpYKU1LHEq3F"
   },
   "outputs": [],
   "source": [
    "class FileProcessingSystem:\n",
    "    \"\"\"\n",
    "    Class for file analysis\n",
    "    \"\"\"\n",
    "    def __init__(self, file, flags, negative_emotions_bool, preferred_gestures_bool, analyzed_segment_len, lang):\n",
    "        \"\"\"\n",
    "        Initializing of file and its params\n",
    "        @param file: FileInfo instance to analyze\n",
    "        @param analyzed_segment_len: length of one file fragment to analyze\n",
    "        @param language_flag: text language flag (for recommendations and statistics)\n",
    "        \"\"\"\n",
    "        self.file_path = file\n",
    "        self.analyzed_segment_len = analyzed_segment_len\n",
    "        self.flags = flags\n",
    "        emotions = [\"Happiness\", \"Anger\", \"Disgust\", \"Neutral\", \"Sadness\", \"Surprise\"]\n",
    "        negative_emotions = []\n",
    "        for i, flag in enumerate(negative_emotions_bool):\n",
    "            if flag:\n",
    "                negative_emotions.append(emotions[i])\n",
    "        self.negative_emotions_bool = negative_emotions_bool\n",
    "        self.preferred_gestures_bool = preferred_gestures_bool\n",
    "        self.timestamps = {}\n",
    "        self.computer_vision = VideoSubsystem(self.file_path, negative_emotions, emotions=flags[\"emotionality\"],\n",
    "                                              gesticulation=flags[\"gestures\"], angle=flags[\"angle\"],\n",
    "                                              gaze=flags[\"glances\"], clothes=flags[\"clothes\"], facial_attrs=flags[\"facial_attrs\"])\n",
    "        self.computer_vision.process_video(duration=analyzed_segment_len)\n",
    "        self.speech_processing = SpeechProcessingSubsystem(self.file_path, negative_emotions_bool,\n",
    "                                                           analyzed_segment_len=analyzed_segment_len, lang=lang)\n",
    "\n",
    "\n",
    "    def save_timestamps_to_db(self, timestamps, type_choice):\n",
    "        \"\"\"\n",
    "        Saves timestamps of low speech rate or high background noise to database\n",
    "        @param timestamps: periods to be saved\n",
    "        @param type_choice: 0 for background noise, 1 for speech rate\n",
    "        \"\"\"\n",
    "        periods = []\n",
    "        for time_period in timestamps:\n",
    "            start_seconds, end_seconds = round(time_period[0]), round(time_period[1])\n",
    "            # transform seconds to time type\n",
    "            start = time(hour=start_seconds // 3600, minute=start_seconds // 60, second=start_seconds % 60)\n",
    "            end = time(hour=end_seconds // 3600, minute=end_seconds // 60, second=end_seconds % 60)\n",
    "            periods.append((start, end))\n",
    "        return periods\n",
    "\n",
    "    def get_transcription(self):\n",
    "        \"\"\"\n",
    "        Translates and saves file transcription\n",
    "        \"\"\"\n",
    "        self.speech_processing.speech_recognition()\n",
    "        return self.speech_processing.cleaned_transcription\n",
    "\n",
    "    def get_emotionality(self):\n",
    "        \"\"\"\n",
    "        Gets emotionality from audio and video subsystems, unites them and saves neutral emotion fraction\n",
    "        \"\"\"\n",
    "        video_emotions = self.computer_vision.get_inappropriate_emotion_percentage()\n",
    "        video_neutral_emotions = self.computer_vision.get_emotions()\n",
    "\n",
    "        try:\n",
    "            audio_emotions, audio_neutral_emotions = self.speech_processing.get_emotionality()\n",
    "        except Exception as e:\n",
    "            audio_emotions = self.computer_vision.get_inappropriate_emotion_percentage()\n",
    "            audio_neutral_emotions = self.computer_vision.get_emotions()\n",
    "        audio_emotions = np.array(audio_emotions)\n",
    "        video_emotions = np.array(video_emotions)\n",
    "        incorrect_emotions_percentage = (2 * video_emotions + audio_emotions) / 3\n",
    "        incorrect_emotions_percentage = np.round(np.array(incorrect_emotions_percentage), 3)\n",
    "        emotions_fraction = round(np.sum(incorrect_emotions_percentage) / len(incorrect_emotions_percentage), 3)\n",
    "\n",
    "        neutral_emotions = (np.array(video_neutral_emotions) + np.array(audio_neutral_emotions)) / 2\n",
    "        neutral_emotions_fraction = round(np.sum(neutral_emotions) / len(neutral_emotions), 3)\n",
    "        self.timestamps[\"emotionality\"] = incorrect_emotions_percentage\n",
    "        # доля нейтральных эмоций во всем видео, доля нежелательных эмоций во всем видео, доли нежелательных эмоций в каждом отрезке\n",
    "        return neutral_emotions_fraction, emotions_fraction, incorrect_emotions_percentage\n",
    "\n",
    "    def get_age(self):\n",
    "        ages = self.computer_vision.get_ages()\n",
    "        return np.mean(ages, axis=0)\n",
    "\n",
    "    def get_gender(self):\n",
    "        genders = self.computer_vision.get_genders()\n",
    "        return max(set(genders), key=genders.count)\n",
    "\n",
    "    def get_ethnicity(self):\n",
    "        ethnicities = self.computer_vision.get_ethnicities()\n",
    "        return max(set(ethnicities), key=ethnicities.count)\n",
    "\n",
    "    def get_filler_words(self):\n",
    "        \"\"\"\n",
    "        Gets filler words and phrases, saves them and their count per minute\n",
    "        \"\"\"\n",
    "        all_filler_words, worst_filler_words = self.speech_processing.get_filler_words()\n",
    "\n",
    "        overall_count = sum(list(all_filler_words.values()))\n",
    "        words_per_minute_percentage = round((overall_count / (self.speech_processing.duration / 60)) / 10, 5)\n",
    "        # доля слов-паразитов во всем видео, все слова-паразиты и их встречаемость, самые частые слова-паразиты и их встречаемость\n",
    "        return words_per_minute_percentage, all_filler_words, worst_filler_words\n",
    "\n",
    "    def get_speech_rate(self):\n",
    "        \"\"\"\n",
    "        Gets and saves intervals with slow speech rate and their percentage\n",
    "        \"\"\"\n",
    "        intervals, fractions, final_fraction = self.speech_processing.get_speech_rate()\n",
    "        self.timestamps[\"speech_rate\"] = fractions\n",
    "        # доля с низким темпом речи во всем видео, доли с низким темпом речи в каждом отрезке, интервалы с низким темпом речи\n",
    "        return final_fraction, fractions, intervals\n",
    "\n",
    "    def get_background_noise(self):\n",
    "        \"\"\"\n",
    "        Gets and saves intervals with high background noise and their percentage\n",
    "        \"\"\"\n",
    "        intervals, fractions, final_fraction = self.speech_processing.get_background_noise()\n",
    "        self.timestamps[\"background_noise\"] = fractions\n",
    "        # доля с высоким фоновым шумом во всем видео, доли с высоким фоновым шумом в каждом отрезке, интервалы с высоким фоновым шумом\n",
    "        return final_fraction, fractions, intervals\n",
    "\n",
    "    def get_intelligibility(self):\n",
    "        \"\"\"\n",
    "        Gets and saves intelligibility estimation\n",
    "        \"\"\"\n",
    "        negative_fractions, negative_index = self.speech_processing.get_intelligibility()\n",
    "        fractions = np.round(1 - negative_fractions, 3)\n",
    "        self.timestamps[\"intelligibility\"] = fractions\n",
    "        # разборчивость речи во всем видео, разборчивость речи в каждом отрезке\n",
    "        return 1 - negative_index, fractions\n",
    "\n",
    "    def get_incorrect_angle(self):\n",
    "        \"\"\"\n",
    "        Gets and saves incorrect angle percentage\n",
    "        \"\"\"\n",
    "        incorrect_angle_fractions = self.computer_vision.get_angle()\n",
    "        incorrect_angle_fractions = np.round(np.array(incorrect_angle_fractions), 3)\n",
    "        incorrect_angle = round(np.sum(incorrect_angle_fractions) / len(incorrect_angle_fractions), 3)\n",
    "        # доля некорректного ракурса во всем видео, доля некорректного ракурса в каждом отрезке\n",
    "        return incorrect_angle, incorrect_angle_fractions\n",
    "\n",
    "    def get_incorrect_glances(self):\n",
    "        \"\"\"\n",
    "        Gets and saves incorrect glances percentage\n",
    "        \"\"\"\n",
    "        incorrect_glance_fractions = self.computer_vision.get_gaze()\n",
    "        incorrect_glance_fractions = np.round(np.array(incorrect_glance_fractions), 3)\n",
    "        incorrect_glance = round(np.sum(incorrect_glance_fractions) / len(incorrect_glance_fractions), 3)\n",
    "        self.timestamps[\"glances\"] = incorrect_glance_fractions\n",
    "        # доля некорректного направления взгляда во всем видео, доля некорректного направления взгляда в каждом отрезке\n",
    "        return incorrect_glance, incorrect_glance_fractions\n",
    "\n",
    "    def get_gestures(self):\n",
    "        \"\"\"\n",
    "        Gets and saves gesticulation level\n",
    "        \"\"\"\n",
    "        gestures = self.computer_vision.get_gestures()\n",
    "        gestures = np.round(np.array(gestures), 3)\n",
    "        final_gesture_fraction = round(np.sum(gestures) / len(gestures), 3)\n",
    "        self.timestamps[\"gestures\"] = gestures\n",
    "        # активность жестикуляции во всем видео, активность жестикуляции в каждом отрезке\n",
    "        return final_gesture_fraction, gestures\n",
    "\n",
    "    def get_clothes(self):\n",
    "        \"\"\"\n",
    "        Gets and saves clothes suitability\n",
    "        \"\"\"\n",
    "        clothes = self.computer_vision.get_clothes_estimation()\n",
    "        # корректность одежды (True - одежда подходит)\n",
    "        return clothes\n",
    "\n",
    "    def draw(self):\n",
    "        \"\"\"\n",
    "        Draw analysis result signatures on video file\n",
    "        \"\"\"\n",
    "        print(self.timestamps)\n",
    "        # indexes of best values for each parameter\n",
    "        optimal_indexes = {\n",
    "            \"background_noise\": 0,\n",
    "            \"speech_rate\": 0,\n",
    "            \"emotionality\": 0,\n",
    "            \"intelligibility\": 2,\n",
    "            \"gestures\": self.preferred_gestures_bool,\n",
    "            \"glances\": 0,\n",
    "            \"lightning\": 0,\n",
    "        }\n",
    "        lst = ORDER\n",
    "        # texts to put into file\n",
    "        text_values = []\n",
    "        # boolean values (True - optimal) for text color\n",
    "        boolean_flags = []\n",
    "        for period_index, name in enumerate(lst):\n",
    "            if name in self.timestamps:\n",
    "                text_values.append([])\n",
    "                boolean_flags.append([])\n",
    "                # get grades for parameter for each file fragment\n",
    "                res = self.timestamps[name]\n",
    "                print(name, res)\n",
    "                # transform grade into text\n",
    "                for value in res:\n",
    "                    text_idx = 0\n",
    "                    if value > CONSTANTS[name][1]:\n",
    "                        text_idx = 2\n",
    "                    elif value > CONSTANTS[name][0]:\n",
    "                        text_idx = 1\n",
    "                    text = DRAW_VALUES[name][text_idx]\n",
    "                    text_values[-1].append(text)\n",
    "                    # append text color\n",
    "                    if name == \"gestures\":\n",
    "                        boolean_flags[-1].append(text_idx in optimal_indexes[name])\n",
    "                    else:\n",
    "                        boolean_flags[-1].append(text_idx == optimal_indexes[name])\n",
    "\n",
    "        # append values on lightning if possible\n",
    "        lightning_numbers = self.computer_vision.get_lightning()\n",
    "        if len(lightning_numbers) > 0:\n",
    "            text_values.append([])\n",
    "            boolean_flags.append([])\n",
    "            for val in lightning_numbers:\n",
    "                text_values[-1].append(DRAW_VALUES[\"lightning\"][val])\n",
    "                boolean_flags[-1].append(val == 1)\n",
    "\n",
    "        draw_res = DrawResults(self.file_path, dist=self.analyzed_segment_len)\n",
    "        # path for temporary file\n",
    "        temp_path = self.file_path[:self.file_path.rfind('.')] + '_temp.' + \\\n",
    "                       self.file_path[self.file_path.rfind('.')+1:]\n",
    "        # file is saved without noise\n",
    "        print(text_values)\n",
    "        print(boolean_flags)\n",
    "        draw_res.draw(temp_path, text_values, boolean_flags,\n",
    "                      self.computer_vision.get_angle_len(), self.computer_vision.get_incorrect_angle_ind())\n",
    "\n",
    "        # unite video and audio\n",
    "        output = mp_editor.VideoFileClip(temp_path)\n",
    "        painted_path = self.file_path[:self.file_path.rfind('.')] + '_painted.' + self.file_path[self.file_path.rfind('.')+1:]\n",
    "        final_duration = output.duration\n",
    "        output_audio = mp_editor.VideoFileClip(self.file_path).audio.subclip(0, final_duration)\n",
    "        output.audio = output_audio\n",
    "        output.write_videofile(painted_path)\n",
    "        return painted_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "1kADWpP-GxmL"
   },
   "outputs": [],
   "source": [
    "flags = {\"emotionality\": True, \"gestures\": True, \"angle\": True, \"glances\": True, \"clothes\": True, \"facial_attrs\": True}\n",
    "negative_emotions = [False, True, True, False, False, False] # Happiness, Anger, Disgust, Neutral, Sadness, Surprise\n",
    "preferred_gestures = [True, True, False] # inactive, medium, active"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Presentation in Russian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JdUiHA6nStg4",
    "outputId": "31ec89e8-441e-4e72-ae76-bdc0c2cc74b5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                                     | 0/6 [00:00<?, ?it/s]I0000 00:00:1745696779.098178 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745696779.201932 6552099 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696779.250288 6552099 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696779.270215 6552112 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n",
      "I0000 00:00:1745696779.532242 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745696779.636194 6552125 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696779.670540 6552134 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745696779.944309 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745696780.052252 6552141 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696780.085338 6552149 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745696780.366005 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745696780.476723 6552166 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696780.508590 6552166 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745696780.789570 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745696780.898298 6552200 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696780.931680 6552200 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745696781.211746 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745696781.319498 6552222 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696781.351406 6552216 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745696781.634619 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745696781.741498 6552240 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696781.773427 6552247 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745696782.052592 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745696782.159954 6552256 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696782.191876 6552256 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745696782.476756 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745696782.597703 6552288 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696782.641710 6552288 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745696782.942902 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745696783.065943 6552298 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696783.100717 6552298 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745696783.384117 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745696783.513021 6552324 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696783.548390 6552329 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745696796.130816 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745696796.131595 6551608 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1745696796.139670 6552532 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696796.155676 6552532 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745696808.108562 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745696808.220442 6552865 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696808.253396 6552865 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      " 17%|████████████████████████████████████▊                                                                                                                                                                                        | 1/6 [00:33<02:47, 33.51s/it]I0000 00:00:1745696812.939953 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745696813.044697 6553043 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696813.078084 6553051 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745696813.365572 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745696813.481541 6553058 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696813.516086 6553067 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745696813.800910 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745696813.912127 6553084 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696813.945204 6553094 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745696814.216335 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745696814.322458 6553101 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696814.353597 6553101 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745696814.740819 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745696814.857055 6553129 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696814.889377 6553140 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745696815.158674 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745696815.264648 6553146 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696815.294597 6553146 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745696815.553475 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745696815.662823 6553175 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696815.694510 6553175 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745696815.967565 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745696816.077276 6553191 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696816.110164 6553191 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745696816.379013 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745696816.493769 6553211 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696816.528543 6553211 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745696816.810908 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745696816.922145 6553244 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696816.954934 6553243 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745696817.227212 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745696817.351176 6553259 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696817.385383 6553259 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745696829.743914 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745696829.744613 6551608 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1745696829.753962 6553511 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696829.771551 6553511 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      " 33%|█████████████████████████████████████████████████████████████████████████▋                                                                                                                                                   | 2/6 [01:02<02:03, 30.77s/it]I0000 00:00:1745696841.200670 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745696841.304995 6553684 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696841.335950 6553688 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745696841.604124 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745696841.716438 6553710 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696841.748184 6553716 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745696842.016173 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745696842.123496 6553726 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696842.154626 6553726 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745696842.425289 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745696842.537100 6553745 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696842.568275 6553745 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745696842.842741 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745696842.950711 6553767 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696842.981775 6553767 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745696843.251845 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745696843.359523 6553786 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696843.392291 6553786 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745696843.661086 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745696843.768045 6553811 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696843.799748 6553811 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745696844.066842 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745696844.175524 6553822 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696844.206710 6553822 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745696844.476048 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745696844.582982 6553852 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696844.613466 6553852 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745696844.887299 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745696844.996942 6553868 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696845.028548 6553873 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745696845.301512 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745696845.425548 6553883 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696845.460520 6553883 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745696858.499777 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745696858.500493 6551608 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1745696858.508622 6554232 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696858.525252 6554232 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      " 50%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                              | 3/6 [01:32<01:31, 30.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max() arg is an empty sequence\n",
      "max() arg is an empty sequence\n",
      "max() arg is an empty sequence\n",
      "max() arg is an empty sequence\n",
      "max() arg is an empty sequence\n",
      "max() arg is an empty sequence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1745696869.921274 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745696870.051002 6554445 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696870.111499 6554445 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745696870.397263 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745696870.511239 6554463 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696870.544067 6554463 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745696870.813873 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745696870.920435 6554496 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696870.952145 6554496 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745696871.234896 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745696871.347678 6554557 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696871.380590 6554567 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745696871.649879 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745696871.759347 6554584 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696871.792187 6554584 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745696872.062674 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745696872.175281 6554604 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696872.207014 6554604 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745696872.477341 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745696872.590077 6554630 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696872.622803 6554630 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745696873.029280 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745696873.159718 6554646 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696873.193576 6554646 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745696873.638057 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745696873.774467 6554679 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696873.809971 6554678 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745696874.100638 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745696874.224879 6554696 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696874.262388 6554703 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745696875.042836 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745696875.221797 6554721 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696875.282649 6554730 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745696899.681005 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745696899.685182 6551608 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1745696899.713749 6555342 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696899.741382 6555351 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      " 67%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                         | 4/6 [02:08<01:05, 32.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max() arg is an empty sequence\n",
      "max() arg is an empty sequence\n",
      "max() arg is an empty sequence\n",
      "max() arg is an empty sequence\n",
      "max() arg is an empty sequence\n",
      "max() arg is an empty sequence\n",
      "max() arg is an empty sequence\n",
      "max() arg is an empty sequence\n",
      "max() arg is an empty sequence\n",
      "max() arg is an empty sequence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1745696906.318765 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745696906.439458 6555456 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696906.473312 6555456 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745696906.834595 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745696906.993465 6555482 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696907.043709 6555492 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745696907.384269 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745696907.504840 6555498 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696907.551385 6555498 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745696907.927178 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745696908.071599 6555528 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696908.120870 6555528 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745696908.483560 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745696908.605341 6555545 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696908.654612 6555548 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745696908.929137 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745696909.043300 6555569 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696909.076975 6555573 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745696909.359283 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745696909.484866 6555587 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696909.523331 6555597 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745696909.807453 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745696909.930006 6555612 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696909.966066 6555612 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745696910.263314 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745696910.389814 6555628 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696910.423756 6555629 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745696910.718414 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745696910.844179 6555652 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696910.878407 6555652 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745696911.180010 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745696911.315019 6555670 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696911.352388 6555670 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745696924.867448 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745696924.868126 6551608 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1745696924.878063 6555855 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696924.894162 6555855 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      " 83%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                    | 5/6 [02:32<00:29, 29.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max() arg is an empty sequence\n",
      "max() arg is an empty sequence\n",
      "max() arg is an empty sequence\n",
      "max() arg is an empty sequence\n",
      "max() arg is an empty sequence\n",
      "max() arg is an empty sequence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1745696928.791612 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745696928.907897 6555921 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696928.942418 6555921 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745696929.233868 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745696929.354708 6555936 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696929.388981 6555936 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745696929.675269 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745696929.799256 6555961 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696929.834542 6555961 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745696930.124463 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745696930.247527 6555979 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696930.281580 6555979 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745696930.584538 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745696930.713489 6556005 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696930.748012 6556005 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745696931.035702 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745696931.165588 6556023 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696931.203664 6556024 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745696931.532118 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745696931.661823 6556088 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696931.696096 6556088 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745696931.759944 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745696931.887791 6556103 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696931.923797 6556113 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745696940.836501 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745696940.839022 6551608 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1745696940.850382 6556306 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745696940.868615 6556317 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [02:47<00:00, 27.88s/it]\n"
     ]
    }
   ],
   "source": [
    "processing = FileProcessingSystem(\"short_rus.mp4\", flags, negative_emotions, preferred_gestures, 10, \"ru\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "vi1QA2enK_XU",
    "outputId": "7a2c07bd-36f2-4241-98e7-3b0a1467430c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5623/5623 [03:44<00:00, 25.03frames/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'привет друзья это успеть за секунд и сергей коростелев сегодня я расскажу что такое пич или техника в лифте представьте себе что вы едете в лифте с биллом гейтсом и вам нужно быстро рассказать о своем бизнесе итак пич первое это название продукта или проекта второе категория продукта то есть что это вообще такое третье целевая аудитория для кого этот проект четвертое основная задача какие цели он преследует и конечно же пятое это его уникальность в чем ваш проект уникален ну к примеру расскажу вот про наш быстро сервис мы сейчас делаем сервис услуги мечты услуги мечты это онлайн сервис по предоставлению бытовых услуг для мам и пап которые позволили заказать множество услуг в одном месте применяя высочайшие стандарты обслуживания и технологии освобождая людям время для реализации мечты и желаний надеюсь я успел'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processing.get_transcription()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39.17979322698467\n",
      "male\n",
      "White\n"
     ]
    }
   ],
   "source": [
    "print(processing.get_age())\n",
    "print(processing.get_gender())\n",
    "print(processing.get_ethnicity())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LG2OBVj5LDjL",
    "outputId": "aa604af6-5628-44e7-968a-d8d59c6a8417"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.649, 0.283, array([0.7, 0.5, 0.3, 0.2, 0. , 0. ]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processing.get_emotionality()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n-ndjKVzLJ8Z",
    "outputId": "4e53884d-b8d2-4839-883e-0c666b82d0dd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.96034,\n",
       " {'в лифте': 2,\n",
       "  'услуги мечты': 2,\n",
       "  'вообще': 1,\n",
       "  'конечно': 1,\n",
       "  'ну': 1,\n",
       "  'вот': 1,\n",
       "  'то есть': 1},\n",
       " {'в лифте': 2, 'услуги мечты': 2})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processing.get_filler_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KvNf1uw1LMFm",
    "outputId": "4361a483-3442-494e-c438-4c089d17411c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, array([0., 0., 0., 0., 0., 0.]), [])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processing.get_speech_rate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GwaDnGwTLT9R",
    "outputId": "0429f3e8-37bc-4e66-a629-b3904f3f84b6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, array([0.045 , 0.0375, 0.0405, 0.043 , 0.053 , 0.042 ]), [])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processing.get_background_noise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oJGLKRZcLVsu",
    "outputId": "98f4a34e-de4e-4577-f37c-c6725fab385e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9652000000000001, array([0.964, 0.97 , 0.968, 0.966, 0.958, 0.966]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processing.get_intelligibility()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kVlShanfLV6j",
    "outputId": "e73d7a9f-1479-49d4-b85b-9cb9bbb3b36f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.128, array([0.347, 0.223, 0.197, 0.   , 0.   , 0.   ]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processing.get_incorrect_angle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hPq4LHc4LWGR",
    "outputId": "b33dc072-77ea-49d7-f7c9-f58415c4bb78"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.633, array([0.378, 0.477, 0.557, 0.384, 1.   , 1.   ]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processing.get_incorrect_glances()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "69_yaU1zLWgz",
    "outputId": "79b42430-bb5d-4029-ead7-dade5e13d27f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.0, array([2, 2, 2, 2, 2, 2]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processing.get_gestures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4jDLDPH1LWsL",
    "outputId": "3cf39a69-e325-4d70-a5a4-4cd36003a9c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processing.get_clothes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HdwxJIdwXXv1",
    "outputId": "bb7134a1-e0b6-4a9a-d3bf-83d46fbecbaf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'emotionality': array([0.7, 0.5, 0.3, 0.2, 0. , 0. ]), 'speech_rate': array([0., 0., 0., 0., 0., 0.]), 'background_noise': array([0.045 , 0.0375, 0.0405, 0.043 , 0.053 , 0.042 ]), 'intelligibility': array([0.964, 0.97 , 0.968, 0.966, 0.958, 0.966]), 'glances': array([0.378, 0.477, 0.557, 0.384, 1.   , 1.   ]), 'gestures': array([2, 2, 2, 2, 2, 2])}\n",
      "background_noise [0.045  0.0375 0.0405 0.043  0.053  0.042 ]\n",
      "speech_rate [0. 0. 0. 0. 0. 0.]\n",
      "emotionality [0.7 0.5 0.3 0.2 0.  0. ]\n",
      "intelligibility [0.964 0.97  0.968 0.966 0.958 0.966]\n",
      "gestures [2 2 2 2 2 2]\n",
      "glances [0.378 0.477 0.557 0.384 1.    1.   ]\n",
      "[['No background noise', 'No background noise', 'No background noise', 'No background noise', 'No background noise', 'No background noise'], ['Optimal speech rate', 'Optimal speech rate', 'Optimal speech rate', 'Optimal speech rate', 'Optimal speech rate', 'Optimal speech rate'], ['Undesired emotions', 'Not fully desired emotions', 'Mostly desired emotions', 'Mostly desired emotions', 'Mostly desired emotions', 'Mostly desired emotions'], ['Speech is fully intelligible', 'Speech is fully intelligible', 'Speech is fully intelligible', 'Speech is fully intelligible', 'Speech is fully intelligible', 'Speech is fully intelligible'], ['Active gesturing', 'Active gesturing', 'Active gesturing', 'Active gesturing', 'Active gesturing', 'Active gesturing'], [None, None, None, None, 'You often look away', 'You often look away'], ['Optimal lighting', 'Optimal lighting', 'Optimal lighting', 'Optimal lighting', 'Optimal lighting', 'Optimal lighting']]\n",
      "[[True, True, True, True, True, True], [True, True, True, True, True, True], [False, False, True, True, True, True], [True, True, True, True, True, True], [False, False, False, False, False, False], [True, True, True, True, False, False], [True, True, True, True, True, True]]\n",
      "Moviepy - Building video short_rus_painted.mp4.\n",
      "MoviePy - Writing audio in short_rus_paintedTEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video short_rus_painted.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready short_rus_painted.mp4\n"
     ]
    }
   ],
   "source": [
    "painted_path = processing.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 503
    },
    "id": "Y2TggR_cuXa8",
    "outputId": "3d40f481-d230-4f0f-fbc5-330166d6e875"
   },
   "outputs": [],
   "source": [
    "video_path = painted_path\n",
    "video_clip = VideoFileClip(video_path)\n",
    "#video_clip.ipython_display(width=640)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Presentation in English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JdUiHA6nStg4",
    "outputId": "31ec89e8-441e-4e72-ae76-bdc0c2cc74b5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                                     | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max() arg is an empty sequence\n",
      "max() arg is an empty sequence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1745699036.187894 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699036.294937 6603310 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699036.337081 6603318 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699036.449063 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699036.562379 6603324 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699036.596850 6603324 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699036.734436 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699036.846833 6603356 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699036.880061 6603356 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699037.056731 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699037.166393 6603374 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699037.199425 6603374 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699037.341527 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699037.450940 6603390 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699037.483217 6603390 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699037.603701 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699037.715592 6603408 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699037.747236 6603408 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699037.863032 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699037.977504 6603434 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699038.010110 6603434 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699038.124824 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699038.233020 6603450 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699038.264472 6603460 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699038.419831 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699038.531577 6603466 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699038.564655 6603472 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699038.691598 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699038.800973 6603492 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699038.832346 6603492 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699038.946629 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699039.061171 6603508 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699039.093668 6603508 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699044.063826 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699044.064533 6551608 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1745699044.072796 6603709 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699044.089725 6603716 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699047.585236 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699047.698987 6603800 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699047.746364 6603800 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      " 11%|████████████████████████▌                                                                                                                                                                                                    | 1/9 [00:13<01:50, 13.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max() arg is an empty sequence\n",
      "max() arg is an empty sequence\n",
      "max() arg is an empty sequence\n",
      "max() arg is an empty sequence\n",
      "max() arg is an empty sequence\n",
      "max() arg is an empty sequence\n",
      "max() arg is an empty sequence\n",
      "max() arg is an empty sequence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1745699049.060853 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699049.159713 6603851 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699049.211140 6603855 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699049.367410 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699049.471026 6603869 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699049.503679 6603875 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699049.587215 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699049.701179 6603884 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699049.733474 6603884 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699049.854776 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699049.963588 6603910 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699049.997194 6603910 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699050.127185 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699050.239446 6603925 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699050.271331 6603925 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699050.400574 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699050.510248 6603945 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699050.542713 6603945 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699050.665885 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699050.775454 6603965 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699050.807282 6603965 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699050.967666 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699051.076113 6603989 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699051.107757 6603994 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699051.286962 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699051.393683 6604004 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699051.426451 6604012 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699051.551138 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699051.659973 6604020 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699051.692122 6604020 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699051.818532 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699051.936257 6604049 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699051.969936 6604053 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699056.893434 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699056.894312 6551608 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1745699056.904703 6604131 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699056.924496 6604131 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      " 22%|█████████████████████████████████████████████████                                                                                                                                                                            | 2/9 [00:23<01:21, 11.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max() arg is an empty sequence\n",
      "max() arg is an empty sequence\n",
      "max() arg is an empty sequence\n",
      "max() arg is an empty sequence\n",
      "max() arg is an empty sequence\n",
      "max() arg is an empty sequence\n",
      "max() arg is an empty sequence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1745699059.228518 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699059.331559 6604176 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699059.366797 6604185 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699059.467623 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699059.573035 6604193 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699059.603992 6604193 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699059.751355 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699059.857784 6604218 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699059.888564 6604218 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699060.017618 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699060.129518 6604232 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699060.163282 6604232 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699060.245208 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699060.353756 6604254 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699060.384562 6604254 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699060.499968 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699060.608892 6604280 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699060.641440 6604280 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699060.724139 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699060.839558 6604311 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699060.873131 6604311 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699060.987380 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699061.094921 6604339 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699061.126407 6604339 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699061.262172 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699061.369541 6604355 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699061.399811 6604355 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699061.480054 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699061.588124 6604371 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699061.618685 6604371 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699061.716845 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699061.834271 6604393 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699061.866584 6604393 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699066.806348 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699066.807534 6551608 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1745699066.817316 6604502 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699066.834145 6604499 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      " 33%|█████████████████████████████████████████████████████████████████████████▋                                                                                                                                                   | 3/9 [00:33<01:04, 10.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max() arg is an empty sequence\n",
      "max() arg is an empty sequence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1745699069.289603 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699069.391813 6604558 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699069.424131 6604558 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699069.540852 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699069.660505 6604582 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699069.695671 6604582 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699069.779983 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699069.895700 6604605 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699069.929834 6604605 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699070.037768 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699070.159048 6604622 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699070.193230 6604622 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699070.321285 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699070.442726 6604640 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699070.480717 6604640 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699070.648707 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699070.774635 6604661 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699070.811833 6604667 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699070.901741 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699071.029295 6604690 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699071.066193 6604699 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699071.195137 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699071.319145 6604719 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699071.354699 6604719 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699071.468524 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699071.593973 6604740 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699071.632760 6604744 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699071.758642 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699071.864937 6604759 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699071.894135 6604759 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699072.008074 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699072.119332 6604775 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699072.151997 6604774 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699076.914500 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699076.915180 6551608 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1745699076.922929 6604872 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699076.938297 6604872 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      " 44%|██████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                          | 4/9 [00:44<00:54, 10.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max() arg is an empty sequence\n",
      "max() arg is an empty sequence\n",
      "max() arg is an empty sequence\n",
      "max() arg is an empty sequence\n",
      "max() arg is an empty sequence\n",
      "max() arg is an empty sequence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1745699080.092742 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699080.203794 6604951 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699080.236254 6604951 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699080.371222 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699080.479885 6604974 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699080.512749 6604980 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699080.643639 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699080.757538 6604992 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699080.790279 6604992 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699080.892711 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699081.000852 6605017 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699081.034663 6605017 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699081.160941 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699081.276270 6605035 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699081.308781 6605035 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699081.441214 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699081.550094 6605050 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699081.582818 6605050 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699081.662330 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699081.773682 6605067 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699081.805100 6605067 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699081.921345 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699082.030076 6605089 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699082.062844 6605089 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699082.181756 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699082.291944 6605107 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699082.323579 6605107 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699082.445539 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699082.558302 6605127 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699082.591761 6605127 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699082.719615 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699082.837079 6605147 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699082.870513 6605147 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699087.859059 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699087.859715 6551608 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1745699087.868739 6605231 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699087.888925 6605231 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      " 56%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                  | 5/9 [00:55<00:43, 10.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max() arg is an empty sequence\n",
      "max() arg is an empty sequence\n",
      "max() arg is an empty sequence\n",
      "max() arg is an empty sequence\n",
      "max() arg is an empty sequence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1745699090.717444 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699090.819181 6605289 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699090.851136 6605289 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699090.932530 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699091.040417 6605308 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699091.075171 6605308 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699091.157737 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699091.266940 6605329 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699091.299419 6605329 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699091.381253 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699091.490447 6605340 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699091.523697 6605340 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699091.647693 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699091.758624 6605357 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699091.789827 6605357 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699091.905515 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699092.012696 6605380 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699092.044196 6605380 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699092.176183 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699092.287146 6605401 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699092.318090 6605401 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699092.399539 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699092.509082 6605419 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699092.541413 6605419 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699092.660898 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699092.772880 6605435 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699092.805674 6605441 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699092.906417 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699093.014469 6605457 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699093.047991 6605457 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699093.128120 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699093.244528 6605475 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699093.277356 6605479 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699098.287528 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699098.288247 6551608 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1745699098.297283 6605577 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699098.313292 6605577 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      " 67%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                         | 6/9 [01:05<00:32, 10.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max() arg is an empty sequence\n",
      "max() arg is an empty sequence\n",
      "max() arg is an empty sequence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1745699101.374698 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699101.476108 6605667 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699101.510173 6605667 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699101.589640 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699101.694088 6605681 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699101.726444 6605681 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699101.838104 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699101.944777 6605705 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699101.976039 6605705 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699102.097982 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699102.203730 6605722 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699102.236281 6605722 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699102.358117 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699102.472956 6605741 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699102.504923 6605741 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699102.582885 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699102.690477 6605766 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699102.721990 6605766 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699102.842787 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699102.950659 6605790 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699102.982150 6605790 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699103.066589 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699103.179273 6605833 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699103.212046 6605833 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699103.293115 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699103.401511 6605854 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699103.432927 6605854 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699103.551280 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699103.660679 6605871 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699103.691787 6605871 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699103.794852 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699103.907997 6605897 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699103.940144 6605897 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699108.940656 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699108.941334 6551608 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1745699108.951427 6606016 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699108.968117 6606016 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      " 78%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                 | 7/9 [01:16<00:21, 10.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max() arg is an empty sequence\n",
      "max() arg is an empty sequence\n",
      "max() arg is an empty sequence\n",
      "max() arg is an empty sequence\n",
      "max() arg is an empty sequence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1745699112.225677 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699112.333661 6606154 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699112.367238 6606154 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699112.447399 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699112.554734 6606170 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699112.585834 6606178 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699112.668348 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699112.776559 6606192 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699112.808044 6606192 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699112.889660 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699112.998449 6606214 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699113.030249 6606214 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699113.163537 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699113.271678 6606228 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699113.303383 6606228 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699113.384007 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699113.492580 6606244 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699113.524296 6606244 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699113.642225 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699113.754119 6606260 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699113.786836 6606260 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699113.867379 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699113.975136 6606285 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699114.007108 6606285 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699114.108420 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699114.216040 6606300 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699114.248495 6606300 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699114.332469 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699114.441152 6606318 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699114.472375 6606318 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699114.557529 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699114.673437 6606350 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699114.705813 6606350 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699119.641422 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699119.642354 6551608 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1745699119.651251 6606435 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699119.677041 6606435 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      " 89%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                        | 8/9 [01:27<00:10, 10.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max() arg is an empty sequence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1745699122.975369 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699123.077579 6606515 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699123.146448 6606518 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699123.231254 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699123.334302 6606532 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699123.365560 6606532 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699123.446291 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699123.555932 6606554 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699123.587508 6606554 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699123.688027 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699123.798074 6606564 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699123.830360 6606564 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699123.913141 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699124.020936 6606587 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699124.052791 6606587 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699124.173398 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699124.281468 6606607 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699124.316681 6606607 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699124.441396 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699124.549975 6606624 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699124.583003 6606624 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699124.709738 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699124.821289 6606643 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699124.853910 6606643 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699124.966397 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699125.075230 6606663 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699125.107440 6606663 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699125.216099 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699125.325306 6606685 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699125.357034 6606681 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699125.392558 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699125.506753 6606697 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699125.539517 6606697 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745699130.186441 6551608 gl_context.cc:369] GL version: 2.1 (2.1 ATI-5.2.4), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1745699130.187688 6551608 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1745699130.198397 6606800 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745699130.221985 6606800 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [01:38<00:00, 10.90s/it]\n"
     ]
    }
   ],
   "source": [
    "processing = FileProcessingSystem(\"short_eng.mp4\", flags, negative_emotions, preferred_gestures, 10, \"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "vi1QA2enK_XU",
    "outputId": "7a2c07bd-36f2-4241-98e7-3b0a1467430c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8944/8944 [03:26<00:00, 43.29frames/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'my fellow americans it has been the honor of my life to serve you i wont stop in fact i will be right there with you as a citizen for all my remaining days but for now whether you are young or whether youre young at heart i do have one final ask of you as your president the same thing i asked when you took a chance on me eight years ago i am asking you to believe not in my ability to bring about change but in yours i am asking you to hold fast to that faith written into our founding documents that idea whispered by slaves and abolitionists that spirit sung by immigrants and homesteaders and those who marched for justice that creed reaffirmed by those who planted flags from foreign battlefields to the surface of the moon a creed at the core of every american whose story is not yet written yes we can yes we did yes we can thank you god bless you may god continue to bless the united states of america thank you thank you thank you thank you thank you thank you thank you thank you thank you thank you thank you'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processing.get_transcription()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58.253327607858836\n",
      "male\n",
      "White\n"
     ]
    }
   ],
   "source": [
    "print(processing.get_age())\n",
    "print(processing.get_gender())\n",
    "print(processing.get_ethnicity())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LG2OBVj5LDjL",
    "outputId": "aa604af6-5628-44e7-968a-d8d59c6a8417"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.395, 0.033, array([0.1, 0. , 0. , 0. , 0. , 0. , 0.1, 0. , 0.1]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processing.get_emotionality()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n-ndjKVzLJ8Z",
    "outputId": "4e53884d-b8d2-4839-883e-0c666b82d0dd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.87209, {'thank you': 12, 'right': 1}, {'thank you': 12})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processing.get_filler_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KvNf1uw1LMFm",
    "outputId": "4361a483-3442-494e-c438-4c089d17411c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), [])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processing.get_speech_rate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GwaDnGwTLT9R",
    "outputId": "0429f3e8-37bc-4e66-a629-b3904f3f84b6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0,\n",
       " array([0.058 , 0.0475, 0.059 , 0.0415, 0.039 , 0.0245, 0.046 , 0.0825,\n",
       "        0.109 ]),\n",
       " [])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processing.get_background_noise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oJGLKRZcLVsu",
    "outputId": "98f4a34e-de4e-4577-f37c-c6725fab385e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7701390578413834,\n",
       " array([0.866, 0.762, 0.753, 0.767, 0.769, 0.78 , 0.763, 0.734, 0.748]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processing.get_intelligibility()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kVlShanfLV6j",
    "outputId": "e73d7a9f-1479-49d4-b85b-9cb9bbb3b36f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.698, array([0.846, 0.475, 0.454, 0.367, 0.562, 0.962, 0.958, 0.929, 0.732]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processing.get_incorrect_angle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hPq4LHc4LWGR",
    "outputId": "b33dc072-77ea-49d7-f7c9-f58415c4bb78"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.555, array([0.633, 0.   , 0.719, 0.604, 0.529, 0.41 , 0.553, 0.649, 0.896]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processing.get_incorrect_glances()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "69_yaU1zLWgz",
    "outputId": "79b42430-bb5d-4029-ead7-dade5e13d27f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.0, array([2, 2, 2, 2, 2, 2, 2, 2, 2]))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processing.get_gestures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4jDLDPH1LWsL",
    "outputId": "3cf39a69-e325-4d70-a5a4-4cd36003a9c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processing.get_clothes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HdwxJIdwXXv1",
    "outputId": "bb7134a1-e0b6-4a9a-d3bf-83d46fbecbaf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'emotionality': array([0.1, 0. , 0. , 0. , 0. , 0. , 0.1, 0. , 0.1]), 'speech_rate': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'background_noise': array([0.058 , 0.0475, 0.059 , 0.0415, 0.039 , 0.0245, 0.046 , 0.0825,\n",
      "       0.109 ]), 'intelligibility': array([0.866, 0.762, 0.753, 0.767, 0.769, 0.78 , 0.763, 0.734, 0.748]), 'glances': array([0.633, 0.   , 0.719, 0.604, 0.529, 0.41 , 0.553, 0.649, 0.896]), 'gestures': array([2, 2, 2, 2, 2, 2, 2, 2, 2])}\n",
      "background_noise [0.058  0.0475 0.059  0.0415 0.039  0.0245 0.046  0.0825 0.109 ]\n",
      "speech_rate [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "emotionality [0.1 0.  0.  0.  0.  0.  0.1 0.  0.1]\n",
      "intelligibility [0.866 0.762 0.753 0.767 0.769 0.78  0.763 0.734 0.748]\n",
      "gestures [2 2 2 2 2 2 2 2 2]\n",
      "glances [0.633 0.    0.719 0.604 0.529 0.41  0.553 0.649 0.896]\n",
      "[['No background noise', 'No background noise', 'No background noise', 'No background noise', 'No background noise', 'No background noise', 'No background noise', 'No background noise', 'No background noise'], ['Optimal speech rate', 'Optimal speech rate', 'Optimal speech rate', 'Optimal speech rate', 'Optimal speech rate', 'Optimal speech rate', 'Optimal speech rate', 'Optimal speech rate', 'Optimal speech rate'], ['Mostly desired emotions', 'Mostly desired emotions', 'Mostly desired emotions', 'Mostly desired emotions', 'Mostly desired emotions', 'Mostly desired emotions', 'Mostly desired emotions', 'Mostly desired emotions', 'Mostly desired emotions'], ['Speech is fully intelligible', 'Speech is fully intelligible', 'Speech is fully intelligible', 'Speech is fully intelligible', 'Speech is fully intelligible', 'Speech is fully intelligible', 'Speech is fully intelligible', 'Speech is fully intelligible', 'Speech is fully intelligible'], ['Active gesturing', 'Active gesturing', 'Active gesturing', 'Active gesturing', 'Active gesturing', 'Active gesturing', 'Active gesturing', 'Active gesturing', 'Active gesturing'], ['You often look away', None, 'You often look away', 'You often look away', None, None, None, 'You often look away', 'You often look away'], ['Too dark lighting', 'Too dark lighting', 'Too dark lighting', 'Too dark lighting', 'Too dark lighting', 'Too dark lighting', 'Too dark lighting', 'Too dark lighting', 'Too dark lighting']]\n",
      "[[True, True, True, True, True, True, True, True, True], [True, True, True, True, True, True, True, True, True], [True, True, True, True, True, True, True, True, True], [True, True, True, True, True, True, True, True, True], [False, False, False, False, False, False, False, False, False], [False, True, False, False, True, True, True, False, False], [False, False, False, False, False, False, False, False, False]]\n",
      "Moviepy - Building video short_eng_painted.mp4.\n",
      "MoviePy - Writing audio in short_eng_paintedTEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video short_eng_painted.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready short_eng_painted.mp4\n"
     ]
    }
   ],
   "source": [
    "painted_path = processing.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 503
    },
    "id": "Y2TggR_cuXa8",
    "outputId": "3d40f481-d230-4f0f-fbc5-330166d6e875"
   },
   "outputs": [],
   "source": [
    "video_path = painted_path\n",
    "video_clip = VideoFileClip(video_path)\n",
    "#video_clip.ipython_display(width=640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
